<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.3 - Generative Neural Networks

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.1 - Generative AI <!-- .element: class="r-fit-text" -->

**Generative AI** refers to models that **learn to generate new data** similar to the training set.

Applications include:
- Image synthesis (e.g., DALLÂ·E, Stable Diffusion)  
- Text generation (e.g., GPT series)  
- Audio synthesis and music  
- Data augmentation

Key idea: learn a distribution $p_\theta(x)$ to generate realistic samples $x \sim p_\theta(x)$.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.2 - Types of Generative Models <!-- .element: class="r-fit-text" -->

Common classes of generative models:

- **Variational Autoencoders (VAEs)**  
  Encode input into latent space $z \sim q_\phi(z|x)$ and decode to reconstruct $\hat{x}$.  

- **Generative Adversarial Networks (GANs)**  
  Train a **generator** $G(z)$ and **discriminator** $D(x)$ in an adversarial game:  
  $$
  \min_G \max_D \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + 
  \mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]
  $$

- **Diffusion Models:** iterative denoising process to generate realistic samples  

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.3 - Variational Autoencoders (VAEs) <!-- .element: class="r-fit-text" -->

VAEs learn a probabilistic mapping from data $x$ to latent variables $z$:

- **Encoder:** $q_\phi(z|x)$ maps input to latent distribution  
- **Decoder:** $p_\theta(x|z)$ reconstructs data from latent code  

Loss function (Evidence Lower Bound, ELBO):

$$
\mathcal{L}_{VAE} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))
$$

- Encourages reconstruction accuracy and latent regularization  
- Latent $z$ can be sampled to generate new data

</section>


<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.4 - Generative Adversarial Networks (GANs) <!-- .element: class="r-fit-text" -->

GANs consist of **two networks**:

- **Generator $G(z)$:** maps random noise $z \sim p_z$ to data space  
- **Discriminator $D(x)$:** distinguishes real from generated samples

Adversarial training:

$$
\min_G \max_D \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + 
\mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]
$$

- Generator improves by fooling the discriminator  
- Discriminator improves by distinguishing real vs generated  
- Produces realistic, high-quality samples

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.5 - Diffusion Models <!-- .element: class="r-fit-text" -->

**Diffusion Models** generate data by **reversing a gradual noising process**.

- **Forward process:** add noise to data over $T$ steps:

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

- **Reverse process:** learn $p_\theta(x_{t-1} | x_t)$ to denoise step by step, reconstructing realistic data:

$$
x_0 \sim p_\theta(x_0 | x_1, \dots, x_T)
$$

Diffusion models produce **high-quality, diverse samples** and are used in image and audio generation.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.6 - Forward & Reverse Process <!-- .element: class="r-fit-text" -->

**Forward (noising) process:**  
$$
x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

**Reverse (denoising) process:**  
Learn a neural network $\epsilon_\theta(x_t, t)$ to predict noise and recover $x_{t-1}$:

$$
x_{t-1} = \frac{1}{\sqrt{1-\beta_t}} \left(x_t - \beta_t \epsilon_\theta(x_t, t) \right) + \sigma_t z
$$

- Iteratively reconstructs $x_0$ from noise $x_T \sim \mathcal{N}(0, I)$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.7 - Applications of Generative Models <!-- .element: class="r-fit-text" -->

Generative models are widely used for EO and general AI tasks:

- **VAEs:** latent space exploration, data compression, synthetic data  
- **GANs:** high-resolution image synthesis, inpainting, anomaly detection  
- **Diffusion Models:** photorealistic image generation, super-resolution, multi-modal EO data

Benefits:
- Mitigate **data scarcity**  
- Improve **robustness of predictive models**  
- Enable **scenario simulation** for rare events

</section>

