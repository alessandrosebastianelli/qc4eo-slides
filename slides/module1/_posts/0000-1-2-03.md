<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.3 - Generative Neural Networks

- [w2.3.1] Generative AI
- [w2.3.2] Types of Generative Models
- [w2.3.3] Variational Autoencoders (VAEs)
- [w2.3.4] Generative Adversarial Networks (GANs)
- [w2.3.5] Diffusion Models
- [w2.3.6] Forward & Reverse Process
- [w2.3.7] Applications of Generative Models

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.1 - Generative AI <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Generative AI</strong> refers to models that <strong>learn to generate new data</strong> similar to the training set. The key idea is to learn a distribution $p_\theta(x)$ that allows generating realistic samples $x \sim p_\theta(x)$.
      </p>
      <p>
        Common applications include:
      </p>
      <ul>
        <li>Image synthesis (e.g., DALLÂ·E, Stable Diffusion)</li>
        <li>Text generation (e.g., GPT series)</li>
        <li>Audio synthesis and music generation</li>
        <li>Data augmentation for machine learning</li>
      </ul>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/373817930/figure/fig1/AS:11431281195229723@1695139673639/An-overview-of-generative-AI-models.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/TNNLS.2023.3323424)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.2 - Types of Generative Models <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Common classes of generative models include:
      </p>
      <ul>
        <li><strong>Variational Autoencoders (VAEs):</strong> Encode input into latent space $z \sim q_\phi(z|x)$ and decode to reconstruct $\hat{x}$.</li>
        <li><strong>Generative Adversarial Networks (GANs):</strong> Train a generator $G(z)$ and discriminator $D(x)$ in an adversarial game:
        $$\min_G \max_D \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]$$</li>
        <li><strong>Diffusion Models:</strong> Use iterative denoising process to generate realistic samples.</li>
      </ul>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/363487937/figure/fig5/AS:11431281084091388@1663019367387/A-summary-of-Generative-Models-and-Discriminative-Models-29.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.13140/RG.2.2.19915.26409)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.3 - Variational Autoencoders (VAEs) <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        VAEs learn a probabilistic mapping from data $x$ to latent variables $z$. The model consists of:
      </p>
      <ul>
        <li><strong>Encoder:</strong> $q_\phi(z|x)$ maps input to latent distribution</li>
        <li><strong>Decoder:</strong> $p_\theta(x|z)$ reconstructs data from latent code</li>
      </ul>
      <p>
        <strong>Loss function (Evidence Lower Bound, ELBO):</strong>
      </p>
      <p>
        $$\mathcal{L}_{VAE} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))$$
      </p>
      <p>
        This encourages reconstruction accuracy and latent regularization. The latent variable $z$ can be sampled to generate new data.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/343786865/figure/fig2/AS:926819213053956@1597913992111/Architecture-of-Variational-Autoencoder-VAE.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/ACCESS.2020.3015569)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.4 - Generative Adversarial Networks (GANs) <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        GANs consist of <strong>two networks</strong> trained in competition:
      </p>
      <ul>
        <li><strong>Generator $G(z)$:</strong> maps random noise $z \sim p_z$ to data space</li>
        <li><strong>Discriminator $D(x)$:</strong> distinguishes real from generated samples</li>
      </ul>
      <p>
        <strong>Adversarial training objective:</strong>
      </p>
      <p>
        $$\min_G \max_D \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]$$
      </p>
      <p>
        The generator improves by fooling the discriminator, while the discriminator improves by distinguishing real versus generated samples. This produces realistic, high-quality outputs.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/331282441/figure/fig1/AS:729118295851009@1550843371347/Basic-structure-of-generative-adversarial-network.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/ACCESS.2019.2905015)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.5 - Diffusion Models <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Diffusion Models</strong> generate data by <strong>reversing a gradual noising process</strong>. The process consists of two phases:
      </p>
      <p>
        <strong>Forward process:</strong> add noise to data over $T$ steps:
      </p>
      <p>
        $$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$$
      </p>
      <p>
        <strong>Reverse process:</strong> learn $p_\theta(x_{t-1} | x_t)$ to denoise step by step:
      </p>
      <p>
        $$x_0 \sim p_\theta(x_0 | x_1, \dots, x_T)$$
      </p>
      <p>
        Diffusion models produce <strong>high-quality, diverse samples</strong> and are used in image and audio generation tasks.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/378106014/figure/fig2/AS:11431281216298025@1709544066088/The-forward-and-reverse-diffusion-process.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/TVCG.2024.3372043)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.6 - Forward & Reverse Process <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Forward (noising) process:</strong>
      </p>
      <p>
        $$x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)$$
      </p>
      <p>
        <strong>Reverse (denoising) process:</strong>
      </p>
      <p>
        Learn a neural network $\epsilon_\theta(x_t, t)$ to predict noise and recover $x_{t-1}$:
      </p>
      <p>
        $$x_{t-1} = \frac{1}{\sqrt{1-\beta_t}} \left(x_t - \beta_t \epsilon_\theta(x_t, t) \right) + \sigma_t z$$
      </p>
      <p>
        The model iteratively reconstructs $x_0$ from pure noise $x_T \sim \mathcal{N}(0, I)$, progressively removing noise to generate high-quality samples.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/381155506/figure/fig1/AS:11431281261178880@1720162746854/Illustration-of-the-Forward-diffusion-and-Reverse-denoising-Process.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1145/3649329.3658228)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.3.7 - Applications of Generative Models <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Generative models are widely used for Earth Observation (EO) and general AI tasks:
      </p>
      <ul>
        <li><strong>VAEs:</strong> latent space exploration, data compression, synthetic data generation</li>
        <li><strong>GANs:</strong> high-resolution image synthesis, inpainting, super-resolution, anomaly detection</li>
        <li><strong>Diffusion Models:</strong> photorealistic image generation, super-resolution, multi-modal EO data synthesis</li>
      </ul>
      <p>
        <strong>Benefits:</strong> Mitigate data scarcity, improve robustness of predictive models, enable scenario simulation for rare events.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/380439652/figure/fig1/AS:11431281255207946@1718720398346/Application-of-Generative-AI-in-Different-Sectors.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/ACCESS.2024.3411606)
      </p>
    </div>
  </div>
</section>