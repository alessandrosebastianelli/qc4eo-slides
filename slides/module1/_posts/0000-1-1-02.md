<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w1.2 - Foundations of Machine Learning <!-- .element: class="r-fit-text" -->

- [w1.2.1] Artificial Intelligence VS Machine Learning VS Deep Learning
- [w1.2.2] Learning methods
- [w1.2.3] The concept of machine learning dataset
- [w1.2.4] Best practices for ML dataset
- [w1.2.5] Artificial Neural Networks
- [w1.2.6] Gradient descent and loss function
- [w1.2.7] Overfitting and Underfitting
- [w1.2.8] Dropout, Regularization & Early Stopping

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.1 - Artificial Intelligence VS Machine Learning VS Deep Learning <!-- .element: class="r-fit-text" -->
  
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://studyopedia.com/wp-content/uploads/2022/12/Data-Science-VS-Machine-Learning-VS-Artificial-Intelligence-vs-Deep-Learning-Studyopedia-1024x432.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://studyopedia.com/data-science/difference-datascience-machinelearing-ai-dl/)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.2 - Learning Methods <!-- .element: class="r-fit-text" -->
  
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://www.techtarget.com/rms/onlineImages/enterpriseai-machine_learning_models_cheat_sheet-f.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.techtarget.com/searchenterpriseai/tip/Types-of-learning-in-machine-learning-explained)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.3 - The concept of machine learning dataset (1) <!-- .element: class="r-fit-text" -->
  

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 60%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
        <p>A machine learning dataset is a collection of data used to train, validate, and test models.</p> 
        <p>It typically consists of input samples (features) and, in supervised learning, their corresponding labels or target values.</p> 
        <p>A dataset is usually split into three parts.</p>
    </div>
    <div style="flex: 0 0 40%; text-align: center;">
      <img 
        src="https://cdn.clickworker.com/wp-content/uploads/2022/07/datesetsplit.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/remote-sensing)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.3 - The concept of machine learning dataset (2) <!-- .element: class="r-fit-text" -->

What turns a *dataset* into a *good dataset*?

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://cdn.clickworker.com/wp-content/uploads/2022/07/gooddataset-1.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.clickworker.com/customer-blog/machine-learning-datasets/)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.4 - Best practices for ML dataset (1) <!-- .element: class="r-fit-text" -->

**k-fold cross-validation** splits data into k parts to train and test the model k times, ensuring more reliable performance estimates.

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://www.aptech.com/wp-content/uploads/2023/05/Blank-diagram-2-1.jpg" 
        style="width: 85%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.aptech.com/blog/understanding-cross-validation/)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.4 - Best practices for ML dataset (2) <!-- .element: class="r-fit-text" -->

**Data augmentation** creates modified copies of training data (e.g., rotations, flips, noise) to improve model robustness and prevent overfitting.

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/ea/ee/data-augmentation-image-augment.png" 
        style="width: 85%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.ibm.com/en-en/think/topics/data-augmentation)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.4 - Best practices for ML dataset (3) <!-- .element: class="r-fit-text" -->

- **Noise robustness** involves intentionally adding noise to training data so the model learns to handle imperfect or noisy inputs more effectively.

- **Class imbalance** avoidance involves using techniques like resampling or weighting to ensure all classes are fairly represented during model training.

- **Feature normalization** ensures that all input features are on a similar scale, helping models train faster and perform more reliably.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.5 - Artificial Neural Networks (1) <!-- .element: class="r-fit-text" -->

Neural networks are machine learning models that mimic the complex functions of the human brain. These models consist of interconnected nodes or neurons that process data, learn patterns and enable tasks such as pattern recognition and decision-making.

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://media.geeksforgeeks.org/wp-content/uploads/20241106171024318092/Artificial-Neural-Networks.webp" 
        style="width: 85%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.geeksforgeeks.org/machine-learning/neural-networks-a-beginners-guide/)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.5 - Artificial Neural Networks (2) <!-- .element: class="r-fit-text" -->

Each neuron computes a **weighted sum of its inputs** and applies a **non-linear activation function** to produce an output:

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
$$

$$
\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})
$$

Where:  
- $\mathbf{a}^{(l-1)}$ = input activations from previous layer  
- $\mathbf{W}^{(l)}$ = weights of the current layer  
- $\mathbf{b}^{(l)}$ = bias term of the current layer  
- $\mathbf{z}^{(l)}$ = linear combination for the current layer  
- $\sigma(\mathbf{z}^{(l)})$ = activation function (e.g., Sigmoid, ReLU, Tanh)  
- $\mathbf{a}^{(l)}$ = output activations of the current layer  

This transformation allows neural networks to learn complex, non-linear patterns from data.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.5 - Artificial Neural Networks (3) <!-- .element: class="r-fit-text" -->

Activation functions introduce non-linearity into neural networks. Common functions:

<div style="
    display: flex; 
    align-items: flex-start; 
    justify-content: center; 
    gap: 3rem;
">
    <div style="flex: 0 0 40%; text-align: justify;">
        <p><strong>Sigmoid:</strong></p>
        $$
        \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z \in \mathbb{R}, \quad \sigma(z) \in (0, 1)
        $$
        <p><strong>Tanh:</strong></p>
        $$
        \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}, \quad z \in \mathbb{R}, \quad \tanh(z) \in (-1, 1)
        $$
        <p><strong>ReLU:</strong></p>
        $$
        \text{ReLU}(z) = \max(0, z), \quad z \in \mathbb{R}, \quad \text{ReLU}(z) \in [0, \infty)
        $$
    </div>
    <div style="flex: 0 0 60%; text-align: center;">
        <img 
            src="https://i0.wp.com/sefiks.com/wp-content/uploads/2020/02/sample-activation-functions-square.png?fit=1372%2C1080&ssl=1" 
            style="width: 90%; border-radius: 10px; margin-top: 1rem;">
        <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
            (image source: https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/)
        </p>
    </div>
</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.6 - Gradient descent and loss function (1) <!-- .element: class="r-fit-text" -->

The **forward pass** computes the output of the neural network by passing inputs through all layers:

$$
\mathbf{a}^{(l)} = \sigma\left(\mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}\right)
$$

Where:  
- $\mathbf{a}^{(l-1)}$ = activations from previous layer  
- $\mathbf{W}^{(l)}, \mathbf{b}^{(l)}$ = weights and biases of current layer  
- $\sigma$ = activation function  
- $\mathbf{a}^{(l)}$ = output of current layer  

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.6 - Gradient descent and loss function (2) <!-- .element: class="r-fit-text" -->

The **backward pass** computes gradients of the loss w.r.t. weights and biases using backpropagation:

$$
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T, \quad
\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}
$$

Weights and biases are updated via **gradient descent**:

$$
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \frac{\partial L}{\partial \mathbf{W}^{(l)}}, \quad
\mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \frac{\partial L}{\partial \mathbf{b}^{(l)}}
$$

Where:  
- $\delta^{(l)}$ = error term at layer $l$  
- $\eta$ = learning rate  

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.6 - Gradient descent and loss function (3) <!-- .element: class="r-fit-text" -->

The **loss function** measures the difference between predictions and true targets.

**Mean Squared Error (MSE):**  
$$
L = \frac{1}{n} \sum_{i=1}^{n} \left(y_i - \hat{y}_i\right)^2
$$

**Cross-Entropy Loss:**  
$$
L = - \frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
$$

Where:  
- $y_i$ = true label  
- $\hat{y}_i$ = predicted output  
- $C$ = number of classes  

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.6 - Gradient descent and loss function (4) <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 100%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/370104769/figure/fig8/AS:11431281150235180@1681877336067/Figure-3-37-Gradient-descent-Algorithm-illustration.ppm" 
        style="width: 85%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://www.researchgate.net/publication/370104769_Nano-Fluids_Convective_Heat_Transfer_Enhancement's_study_and_Neural-Network_Accelerated_Nusselt_Number_Prediction)
      </p>
    </div>
  </div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.7 - Overfitting & Underfitting <!-- .element: class="r-fit-text" -->

- **Underfitting:** Model is too simple to capture data patterns.  
  - High training and validation error.  

- **Overfitting:** Model learns training data too well, including noise.  
  - Low training error but high validation error.  

- Goal: Achieve a balance for good generalization.

</section>


<!-- ============================================================================ -->

<section data-transition="none">

### w1.2.8 - Dropout, Regularization & Early Stopping <!-- .element: class="r-fit-text" -->

- **Dropout:** Randomly deactivate neurons during training to prevent co-adaptation.  

- **L1 / L2 Regularization:** Add penalty to weights to reduce overfitting:  
  $$
  L_\text{reg} = L + \lambda \sum |w| \quad \text{(L1)}, \quad
  L_\text{reg} = L + \lambda \sum w^2 \quad \text{(L2)}
  $$

- **Batch Normalization:** Normalize layer inputs to improve training speed and stability:  
  $$
  \hat{\mathbf{z}}^{(l)} = \frac{\mathbf{z}^{(l)} - \mu_\text{batch}}{\sqrt{\sigma_\text{batch}^2 + \epsilon}}, \quad
  \mathbf{a}^{(l)} = \gamma \hat{\mathbf{z}}^{(l)} + \beta
  $$

- **Early Stopping:** Stop training when validation performance stops improving to prevent overfitting.  

- These techniques improve generalization, robustness, and convergence.

</section>
