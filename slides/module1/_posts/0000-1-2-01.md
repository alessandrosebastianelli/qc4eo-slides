<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.1 - Convolutional Neural Networks

- [w2.1.1] Introduction to CNNs
- [w2.1.2] Convolution Operation
- [w2.1.3] Feature Maps and Channels
- [w2.1.4] Stride, Padding, and Output Size
- [w2.1.5] Pooling Layers
- [w2.1.6] Activation in CNNs
- [w2.1.7] CNN Architecture
- [w2.1.8] Advanced CNN Techniques

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.1 - Convolutional Neural Networks <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Convolutional Neural Networks (CNNs)</strong> are specialized deep learning architectures designed to process spatial or grid-like data such as images or time series. They exploit <strong>local connectivity</strong> and <strong>weight sharing</strong>, enabling efficient feature extraction and reducing the number of parameters compared to fully connected networks.
      </p>
      <p>
        Main components: convolutional layers, activation functions, pooling layers, and fully connected layers.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Wikimedia Commons - https://commons.wikimedia.org/wiki/File:Typical_cnn.png)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.2 - Convolution Operation <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        A <strong>convolutional layer</strong> applies a filter (kernel) that slides over the input to compute a feature map:
      </p>
      <p>
        $$y_{i,j}^{(k)} = \sum_{m} \sum_{n} x_{i+m,\,j+n} \, w_{m,n}^{(k)} + b^{(k)}$$
      </p>
      <p>
        Where $x$ is the input feature map, $w^{(k)}$ is the kernel for the $k$-th feature, $b^{(k)}$ is the bias term, and $y^{(k)}$ is the resulting feature map. Each kernel learns to detect specific local patterns such as edges or textures.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://miro.medium.com/v2/resize:fit:1400/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Medium - Convolution operation visualization)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.3 - Feature Maps and Channels <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        In images, each pixel has multiple <strong>channels</strong> (e.g., RGB for color images). A CNN learns multiple <strong>feature maps</strong> across these channels to extract different patterns.
      </p>
      <p>
        For layer $l$:
      </p>
      <p>
        $$a_{i,j}^{(k,l)} = \sigma\!\left(\sum_{c=1}^{C_{l-1}} (w_{c}^{(k,l)} * a^{(c,l-1)})_{i,j} + b^{(k,l)}\right)$$
      </p>
      <p>
        Where $C_{l-1}$ is the number of input channels, $*$ is the convolution operator, and $\sigma$ is the activation function. This integrates information across spatial and channel dimensions.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/342260255/figure/fig2/AS:903845665861632@1592483129255/Illustration-of-the-convolution-operation-with-multiple-channels-For-example-a-filter.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/TGRS.2020.2990431)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.4 - Stride, Padding, and Output Size <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Stride ($s$)</strong> controls how far the filter moves at each step, and <strong>Padding ($p$)</strong> preserves spatial dimensions by adding borders around the input.
      </p>
      <p>
        The output size $(H_\text{out}, W_\text{out})$ is calculated as:
      </p>
      <p>
        $$H_\text{out} = \frac{H_\text{in} - K + 2p}{s} + 1, \quad W_\text{out} = \frac{W_\text{in} - K + 2p}{s} + 1$$
      </p>
      <p>
        Choosing $p = \frac{K-1}{2}$ yields "same convolution", keeping dimensions constant when $s=1$.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://miro.medium.com/v2/resize:fit:1200/1*1VJDP6qDY9-ExTuQVEOlVg.gif" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Medium - Stride and padding visualization)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.5 - Pooling Layers <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Pooling layers</strong> reduce spatial dimensions while preserving key information, helping to control overfitting and provide translational invariance.
      </p>
      <p>
        <strong>Max pooling:</strong>
        $$y_{i,j} = \max_{(m,n) \in R(i,j)} x_{m,n}$$
      </p>
      <p>
        <strong>Average pooling:</strong>
        $$y_{i,j} = \frac{1}{|R(i,j)|} \sum_{(m,n) \in R(i,j)} x_{m,n}$$
      </p>
      <p>
        Pooling reduces computational requirements and makes the network more robust to small translations in the input.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/333593451/figure/fig3/AS:765890883735552@1559815500875/Max-pooling-and-average-pooling.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/app9112396)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.6 - Activation in CNNs <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        After each convolution, a <strong>non-linear activation</strong> function introduces complexity and prevents the model from collapsing into a linear mapping.
      </p>
      <p>
        Common activation functions:
      </p>
      <ul>
        <li><strong>ReLU:</strong> $\text{ReLU}(z) = \max(0, z)$</li>
        <li><strong>Leaky ReLU:</strong> $\text{LeakyReLU}(z) = \max(\alpha z, z)$</li>
        <li><strong>Tanh, Sigmoid:</strong> used less often in modern CNNs</li>
      </ul>
      <p>
        Activations help extract and combine hierarchical features from raw data.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://miro.medium.com/v2/resize:fit:1400/1*XxxiA0jJvPrHEJHD4z893g.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Medium - Common activation functions)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.7 - CNN Architecture <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        A typical CNN combines multiple layers in sequence:
      </p>
      <ol>
        <li><strong>Convolutional layers</strong> — local feature extraction</li>
        <li><strong>Activation functions</strong> — introduce non-linearity</li>
        <li><strong>Pooling layers</strong> — spatial dimension reduction</li>
        <li><strong>Fully connected layers</strong> — integrate learned features</li>
        <li><strong>Output layer</strong> — produces class scores or predictions</li>
      </ol>
      <p>
        <strong>Forward pass (simplified):</strong>
        $$\mathbf{a}^{(l)} = f_\text{pool}\big(\sigma(\mathbf{W}^{(l)} * \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})\big)$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/342347320/figure/fig1/AS:904857630650371@1592541319634/An-example-of-CNN-architecture.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/ACCESS.2020.3003191)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.1.8 - Advanced CNN Techniques <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Modern CNNs use regularization and optimization strategies to improve performance and generalization:
      </p>
      <ul>
        <li><strong>Batch Normalization:</strong> 
        $$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta$$
        Stabilizes and accelerates training.</li>
        <li><strong>Dropout:</strong> randomly sets activations to zero during training to prevent overfitting.</li>
        <li><strong>Data Augmentation:</strong> expands dataset diversity through transformations.</li>
        <li><strong>Early Stopping:</strong> halts training when validation loss stops improving.</li>
      </ul>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/335845265/figure/fig2/AS:803743942262785@1568651155146/Illustration-of-the-dropout-mechanism-Dropout-randomly-drops-units-along-with-their.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/s19194205)
      </p>
    </div>
  </div>
</section>