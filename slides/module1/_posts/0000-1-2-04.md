<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.4 - Emerging Trends of AI for Earth Observation

- [w2.4.1] AI for Earth Observation (AI4EO)
- [w2.4.2] CNNs for Spatial Analysis
- [w2.4.3] RNNs & Transformers for Temporal Patterns
- [w2.4.4] Generative Models for EO
- [w2.4.5] Foundation Models for Earth Observation
- [w2.4.6] Hybrid and Multi-Modal AI4EO
- [w2.4.7] AI4EO Use Cases
- [w2.4.8] Future Directions

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.1 - AI for Earth Observation (AI4EO) <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>AI4EO</strong> leverages advanced machine learning to extract knowledge from satellite and aerial data. Emerging trends include:
      </p>
      <ul>
        <li><strong>Feature extraction:</strong> CNNs for land cover, vegetation, and urban mapping</li>
        <li><strong>Temporal modeling:</strong> RNNs and Transformers for time series analysis</li>
        <li><strong>Generative models:</strong> Diffusion Models, GANs, and VAEs for synthetic data generation</li>
        <li><strong>Foundation models:</strong> Pre-trained large-scale models for diverse EO tasks</li>
      </ul>
      <p>
        Goal: <strong>accurate, scalable, and interpretable Earth observation insights</strong>.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/364166829/figure/fig1/AS:11431281094758408@1667322568997/Deep-learning-in-remote-sensing-a-comprehensive-review-and-list-of-resources.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.isprsjprs.2019.04.015)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.1 - AI4EO References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Ma, L., et al. (2019). Deep learning in remote sensing applications: A meta-analysis and review. <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 152, 166-177. [Highly cited: 4000+ citations]</li>
      <li>Yuan, Q., et al. (2024). Advancing horizons in remote sensing: a comprehensive survey of deep learning models. <em>Neural Computing and Applications</em>. DOI:10.1007/s00521-024-10165-7</li>
      <li>Xiong, Z., et al. (2022). EarthNets: Empowering AI in Earth Observation. <em>arXiv preprint arXiv:2210.04936</em>.</li>
      <li>Ghamisi, P., et al. (2024). Responsible AI for Earth Observation. <em>arXiv preprint arXiv:2405.20868</em>.</li>
      <li>Del Rosso, M. P., Sebastianelli, A., & Ullo, S. L. (eds.) (2021). <em>Artificial Intelligence Applied to Satellite-based Remote Sensing Data for Earth Observation</em>. IET Telecommunications Series.</li>
      <li>Ullo, S. L., et al. (2022). Urban Sprawl and COVID-19 Impact Analysis by Integrating Deep Learning with Google Earth Engine. <em>Remote Sensing</em>, 14(9), 2038.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.2 - CNNs for Spatial Analysis <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        CNNs are widely used to analyze <strong>spatial patterns</strong> in EO data for tasks including:
      </p>
      <ul>
        <li>Land cover and land use classification</li>
        <li>Object detection (ships, vehicles, buildings)</li>
        <li>Change detection from multi-temporal imagery</li>
        <li>Semantic segmentation of Earth surface features</li>
      </ul>
      <p>
        <strong>Forward pass in EO CNN:</strong>
      </p>
      <p>
        $$\mathbf{a}^{(l)} = f_\text{pool}\big(\sigma(\mathbf{W}^{(l)} * \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})\big)$$
      </p>
      <p>
        CNNs efficiently learn <strong>hierarchical spatial features</strong> from high-resolution satellite images.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/367806056/figure/fig2/AS:11431281109922834@1675270449806/The-procedures-of-land-cover-classification-using-convolutional-neural-networks.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/rs15030735)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.2 - CNNs References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. <em>Advances in neural information processing systems</em>, 25. [Highly cited: 130,000+ citations]</li>
      <li>Janga, B., et al. (2024). A Review of Practical AI for Remote Sensing in Earth Sciences. <em>Remote Sensing</em>, 16(9), 2343. DOI:10.3390/rs16092343</li>
      <li>Cheng, G., et al. (2024). Remote Sensing Object Detection in the Deep Learning Era. <em>Remote Sensing</em>, 16(2), 327. DOI:10.3390/rs16020327</li>
      <li>Kattenborn, T., et al. (2021). Review on Convolutional Neural Networks (CNN) in vegetation remote sensing. <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 173, 24-49.</li>
      <li>Sebastianelli, A., et al. (2022). A speckle filter for Sentinel-1 SAR Ground Range Detected data based on Residual Convolutional Neural Networks. <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>.</li>
      <li>Gamba, P. (Editor-in-Chief since 2023). <em>IEEE Geoscience and Remote Sensing Magazine</em> - Key contributions to AI for remote sensing object detection and urban mapping.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.3 - RNNs & Transformers for Temporal Patterns <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        EO data is often <strong>time series</strong> (e.g., vegetation indices, atmospheric measurements). Sequential models capture temporal dependencies:
      </p>
      <p>
        <strong>RNNs / LSTMs / GRUs:</strong> model sequential dependencies
      </p>
      <p>
        $$h_t = \sigma_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$
      </p>
      <p>
        <strong>Transformers:</strong> capture long-range dependencies using self-attention
      </p>
      <p>
        $$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$$
      </p>
      <p>
        Applications: crop monitoring, flood prediction, climate trend analysis, deforestation detection.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/371737829/figure/fig1/AS:11431281155968020@1685628893894/Framework-of-the-proposed-method-for-crop-classification-using-time-series-remote-sensing.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/TGRS.2023.3282940)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.3 - Temporal Models References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Vaswani, A., et al. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, 30. [Highly cited: 120,000+ citations]</li>
      <li>Rußwurm, M., & Körner, M. (2020). Self-attention for raw optical satellite time series classification. <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 169, 421-435.</li>
      <li>Yuan, Y., & Lin, L. (2021). Self-supervised pretraining of transformers for satellite image time series classification. <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 14, 474-487.</li>
      <li>Xiong, Z., et al. (2022). EarthNets: Empowering AI in Earth Observation. <em>arXiv preprint arXiv:2210.04936</em>.</li>
      <li>Sebastianelli, A., et al. (2021). On Circuit-Based Hybrid Quantum Neural Networks for Remote Sensing Imagery Classification. <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.4 - Generative Models for EO <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Generative AI can <strong>synthesize high-quality EO data</strong>, augment datasets, and enhance training:
      </p>
      <ul>
        <li><strong>Diffusion Models:</strong> iterative denoising to generate realistic satellite imagery
        $$x_{t-1} = \frac{1}{\sqrt{1-\beta_t}} \left(x_t - \beta_t \epsilon_\theta(x_t, t) \right) + \sigma_t z$$</li>
        <li><strong>GANs:</strong> create synthetic multispectral or SAR images</li>
        <li><strong>VAEs:</strong> generate diverse samples for rare events</li>
      </ul>
      <p>
        <strong>Benefits:</strong> Mitigate data scarcity, improve model robustness, enable scenario simulation for rare events.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/377452653/figure/fig1/AS:11431281226637318@1706547821024/Overview-of-the-diffusion-model-based-generative-network-for-remote-sensing-image.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/LGRS.2024.3355123)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.4 - Generative Models References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. <em>arXiv preprint arXiv:1312.6114</em>. [Highly cited: 25,000+ citations]</li>
      <li>Goodfellow, I. J., et al. (2014). Generative adversarial nets. <em>Advances in neural information processing systems</em>, 27. [Highly cited: 65,000+ citations]</li>
      <li>Ho, J., et al. (2020). Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em>, 33, 6840-6851. [Highly cited: 8,000+ citations]</li>
      <li>Chen, K., et al. (2023). Generating Multispectral Satellite Images via Denoising Diffusion Probabilistic Models. <em>IEEE Geoscience and Remote Sensing Letters</em>, 20.</li>
      <li>Di Stasio, P., Sebastianelli, A., et al. (2023). Early Detection of Volcanic Eruption through Artificial Intelligence. <em>IEEE MetroXRAINE</em>.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.5 - Foundation Models for Earth Observation <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Foundation Models (FMs)</strong> are large-scale, pre-trained models that can be adapted to diverse downstream EO tasks with minimal additional training. Recent breakthroughs include:
      </p>
      <ul>
        <li><strong>Prithvi (NASA-IBM):</strong> Geospatial foundation model trained on HLS satellite data</li>
        <li><strong>Prithvi-WxC:</strong> Weather and climate foundation model</li>
        <li><strong>Vision Transformers (ViT):</strong> Adapted for remote sensing</li>
        <li><strong>Multi-modal models:</strong> Combining optical, SAR, and ancillary data</li>
      </ul>
      <p>
        FMs dramatically reduce training requirements and enable zero-shot or few-shot learning for new tasks.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/384579914/figure/fig1/AS:11431281268711432@1726042806896/The-key-categories-of-foundation-models-in-remote-sensing.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.48550/arXiv.2410.16602)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.5 - Foundation Models References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Zhang, X., et al. (2024). Foundation Models for Remote Sensing and Earth Observation: A Survey. <em>arXiv preprint arXiv:2410.16602</em>.</li>
      <li>Jakubik, J., et al. (2023). Foundation Models for Generalist Geospatial Artificial Intelligence. <em>NASA Technical Reports Server</em>.</li>
      <li>Chen, K., et al. (2025). When Remote Sensing Meets Foundation Model: A Survey and Beyond. <em>Remote Sensing</em>, 17(2), 179. DOI:10.3390/rs17020179</li>
      <li>NASA & IBM (2024). Prithvi Geospatial Foundation Model. Available at: https://huggingface.co/ibm-nasa-geospatial</li>
      <li>Reed, C., et al. (2024). Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning. <em>ICCV 2023</em>.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.6 - Hybrid and Multi-Modal AI4EO <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Emerging trend: <strong>hybrid models</strong> combining spatial, temporal, and generative approaches for comprehensive EO analysis.
      </p>
      <ul>
        <li><strong>CNN + RNN/Transformer:</strong> extract spatial features and model temporal dynamics</li>
        <li><strong>CNN + Diffusion/GANs:</strong> generate synthetic multi-temporal or multi-sensor data</li>
        <li><strong>Multi-modal fusion:</strong> integrate optical, SAR, LiDAR, and climate data</li>
        <li><strong>Physics-informed AI:</strong> incorporate physical models with neural networks</li>
      </ul>
      <p>
        Goal: integrate <strong>heterogeneous EO data</strong> in a unified framework for comprehensive Earth system understanding.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/367993399/figure/fig1/AS:11431281110713382@1676024736691/The-framework-of-the-proposed-multimodal-fusion-network-for-semantic-segmentation-of.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/rs15041158)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.6 - Hybrid Models References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Xiong, Z., et al. (2022). EarthNets: Empowering AI in Earth Observation. <em>arXiv preprint arXiv:2210.04936</em>.</li>
      <li>Hong, D., et al. (2021). More Diverse Means Better: Multimodal Deep Learning Meets Remote-Sensing Imagery Classification. <em>IEEE Transactions on Geoscience and Remote Sensing</em>, 59(5), 4340-4354.</li>
      <li>Audebert, N., et al. (2019). Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks. <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 140, 20-32.</li>
      <li>Ghamisi, P., et al. (2024). Responsible AI for Earth Observation. <em>arXiv preprint arXiv:2405.20868</em>.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.7 - AI4EO Use Cases <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Representative applications demonstrating the integration of AI models for Earth observation:
      </p>
      <ul>
        <li><strong>Land cover and urban expansion monitoring</strong> using CNNs</li>
        <li><strong>Crop yield prediction and drought monitoring</strong> with time-series models</li>
        <li><strong>Flood, wildfire, and disaster detection</strong> using multi-modal fusion</li>
        <li><strong>Climate modeling and carbon stock estimation</strong> with foundation models</li>
        <li><strong>Data augmentation and anomaly detection</strong> using generative models</li>
        <li><strong>Super-resolution and gap-filling</strong> for incomplete satellite data</li>
      </ul>
      <p>
        These applications combine all previously introduced AI models for improved performance and insights.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/378553048/figure/fig2/AS:11431281229860879@1708441506095/Overview-of-the-most-common-AI-and-EO-applications.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/rs16040715)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.7 - Use Cases References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Janga, B., et al. (2024). A Review of Practical AI for Remote Sensing in Earth Sciences. <em>Remote Sensing</em>, 16(9), 2343. DOI:10.3390/rs16092343</li>
      <li>Xiong, Z., et al. (2022). EarthNets: Empowering AI in Earth Observation. <em>arXiv preprint arXiv:2210.04936</em>.</li>
      <li>Zhu, X. X., et al. (2017). Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources. <em>IEEE Geoscience and Remote Sensing Magazine</em>, 5(4), 8-36. [Highly cited: 3,000+ citations]</li>
      <li>Yuan, Q., et al. (2020). Deep learning in environmental remote sensing: Achievements and challenges. <em>Remote Sensing of Environment</em>, 241, 111716.</li>
    </ul>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.8 - Future Directions <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Emerging trends shaping the future of AI4EO:
      </p>
      <ul>
        <li><strong>Foundation models for EO:</strong> Pre-trained multi-modal Transformers (Prithvi, Scale-MAE)</li>
        <li><strong>Diffusion-based simulation:</strong> Scenario generation for climate and disaster modeling</li>
        <li><strong>Vision-Language Models:</strong> Natural language queries for EO data (e.g., "show me deforestation in 2024")</li>
        <li><strong>Explainable AI (XAI):</strong> Increase trust and interpretability in EO predictions</li>
        <li><strong>Edge AI:</strong> On-board satellite processing for real-time Earth monitoring</li>
        <li><strong>Quantum-enhanced ML:</strong> Exploring quantum computing for large-scale EO analysis</li>
      </ul>
      <p>
        Goal: <strong>scalable, accurate, and sustainable Earth observation intelligence</strong>.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/382178398/figure/fig10/AS:11431281247743011@1719996706316/Vision-of-future-trends-in-AI-powered-remote-sensing.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/rs16132394)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.4.8 - Future Directions References <!-- .element: class="r-fit-text" -->

  <div style="text-align: left; font-size: 0.75em; padding: 2rem;">
    <p><strong>Key References:</strong></p>
    <ul>
      <li>Ghamisi, P., et al. (2024). Responsible AI for Earth Observation. <em>arXiv preprint arXiv:2405.20868</em>.</li>
      <li>Zhang, X., et al. (2024). Foundation Models for Remote Sensing and Earth Observation: A Survey. <em>arXiv preprint arXiv:2410.16602</em>.</li>
      <li>Chen, K., et al. (2025). When Remote Sensing Meets Foundation Model: A Survey and Beyond. <em>Remote Sensing</em>, 17(2), 179.</li>
      <li>NASA-ESA Workshop (2025). AI Foundation Models for Earth Observation. May 5-7, 2025, Frascati, Italy.</li>
      <li>Tuia, D., et al. (2023). Toward a Collective Agenda on AI for Earth Science Data Analysis. <em>IEEE Geoscience and Remote Sensing Magazine</em>, 11(2), 88-104.</li>
    </ul>
  </div>
</section>