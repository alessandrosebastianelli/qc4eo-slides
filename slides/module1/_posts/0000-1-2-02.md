<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.2 - Recurrent Neural Networks

- [w2.2.1] Introduction to RNNs
- [w2.2.2] The Recurrent Connection
- [w2.2.3] Unfolding Through Time
- [w2.2.4] Vanishing and Exploding Gradients
- [w2.2.5] LSTM Networks
- [w2.2.6] GRU Networks
- [w2.2.7] Sequence Output Types
- [w2.2.8] From RNNs to Transformers
- [w2.2.9] Self-Attention Mechanism
- [w2.2.10] Multi-Head Attention
- [w2.2.11] Transformer Architecture

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.1 - Recurrent Neural Networks <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Recurrent Neural Networks (RNNs)</strong> are designed to process <strong>sequential data</strong>, where the order of elements matters, such as time series, text, or audio signals. RNNs maintain a <strong>hidden state</strong> that captures information from previous time steps, allowing the network to model temporal dependencies.
  </p>
  <div style="flex: 0 0 50%; text-align: center;">
    <img 
      src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/unrolled-rnn_0.png" 
      style="width: 90%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
      (image source: https://builtin.com/data-science/recurrent-neural-networks-and-lstm)
    </p>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.2 - The Recurrent Connection <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        At each time step $t$, the RNN updates its hidden state using the current input $x_t$ and the previous hidden state $h_{t-1}$:
      </p>
      <p>
        $$h_t = \sigma_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$
      </p>
      <p>
        The output is computed as:
      </p>
      <p>
        $$y_t = \sigma_y(W_{hy} h_t + b_y)$$
      </p>
      <p>
        Where $x_t$ is the input at time $t$, $h_t$ is the hidden state, $y_t$ is the output, and $\sigma_h, \sigma_y$ are activation functions such as $\tanh$ or $\text{softmax}$.
      </p>
    </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.3 - Unfolding Through Time <!-- .element: class="r-fit-text" -->

  <p style="text-align: justify;">
    An RNN can be visualized as the same network <strong>unrolled through time</strong> for $T$ steps:
  </p>
  <img 
    src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/unrolled-rnn_0.png" 
    style="width: 70%; border-radius: 10px;">
  <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: https://builtin.com/data-science/recurrent-neural-networks-and-lstm)
  </p>
  <p style="text-align: justify;">
    $$h_t = f(h_{t-1}, x_t; \theta)$$
  </p>
  <p style="text-align: justify;">
    This creates a computational graph where parameters $\theta = \{W_{xh}, W_{hh}, W_{hy}\}$ are <strong>shared across time</strong>. Each step passes information to the next, allowing temporal dependencies to propagate through the sequence.
  </p>
  <p style="text-align: justify;">
    However, this structure leads to <strong>vanishing or exploding gradients</strong> during training, limiting the ability to learn long-term dependencies.
  </p>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.4 - Vanishing and Exploding Gradients <!-- .element: class="r-fit-text" -->

  <div style="text-align: justify;">
    <h3 class="r-fit-text"></h3>
    <p>
      During <strong>backpropagation through time (BPTT)</strong>, gradients are multiplied across time steps:
    </p>
    <p>
      $$\frac{\partial L}{\partial W} = \sum_t \frac{\partial L_t}{\partial h_t} \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}}$$
    </p>
    <p>
      If eigenvalues of $\frac{\partial h_k}{\partial h_{k-1}}$ are less than 1, gradients vanish; if greater than 1, gradients explode. This limits the ability of standard RNNs to learn <strong>long-term dependencies</strong>.
    </p>
    <p>
      Solutions include gradient clipping for exploding gradients, and specialized architectures like LSTM and GRU for vanishing gradients.
    </p>
  </div>
  
  </section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.5 - LSTM Networks (1) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Long Short-Term Memory (LSTM)</strong> networks mitigate gradient issues using <strong>gates</strong> to control information flow.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
        $$\begin{aligned}
        f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
        i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
        \tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \\
        C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
        o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
        h_t &= o_t \odot \tanh(C_t)
        \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://thorirmar.com/post/insight_into_lstm/featured.png" 
        style="width: 99%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://thorirmar.com/post/insight_into_lstm/)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    LSTMs preserve long-term memory via the <strong>cell state</strong> $C_t$ and gated updates, enabling learning of dependencies across longer sequences.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.5 - LSTM Networks (2) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Long Short-Term Memory (LSTM)</strong> networks mitigate gradient issues using <strong>gates</strong> to control information flow.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
      $$\begin{aligned}
      \text{Forget gate: } & f_t \\
      \text{Input gate: } & i_t \\
      \text{Candidate cell state: } & \tilde{C}_t  \\
      \text{Cell state: } & C_t \\
      \text{Output gate: } & o_t \\
      \text{Hidden state: } & h_t
      \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://thorirmar.com/post/insight_into_lstm/featured.png" 
        style="width: 99%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://thorirmar.com/post/insight_into_lstm/)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    $f_t$ decides what to forget from the previous memory, $i_t$ controls the ne information to store, $\tilde{C}$ new candidate content for memory, $C_t$ updated long term memory of LSTM, $o_t$ controls what part of the cell state is output, $h_t$ output of LSTM for current timestep.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.6 - GRU Networks (1) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Gated Recurrent Units (GRUs)</strong> simplify the LSTM architecture by merging the forget and input gates into a single update gate.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        $$\begin{aligned}
        z_t &= \sigma(W_z [h_{t-1}, x_t]) \\
        r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
        \tilde{h}_t &= \tanh(W_h [r_t \odot h_{t-1}, x_t]) \\
        h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
        \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnNORKUR4B90lrIt8zoVCoVRkyGXtyCbtcZw&s" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: DOI:10.13140/RG.2.2.15743.46240)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    GRUs are computationally faster and require fewer parameters while maintaining comparable performance to LSTMs, making them efficient for many sequence modeling tasks.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.6 - GRU Networks (2) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Gated Recurrent Units (GRUs)</strong> simplify the LSTM architecture by merging the forget and input gates into a single update gate.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
      $$\begin{aligned}
      \text{Update gate: } & z_t \\
      \text{Reset gate: } & r_t \\
      \text{Candidate hidden state: } & \tilde{h}_t \\
      \text{Hidden state: } & h_t
      \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnNORKUR4B90lrIt8zoVCoVRkyGXtyCbtcZw&s" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: DOI:10.13140/RG.2.2.15743.46240)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    $z_t$ controls how much of the previous hidden steate to keep, $r_t$ determines how to combine new input with previous memory, $\tilde{h}$ new candidate content for hidden state, $h_t$ final hidden state combining old and new information.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.7 - Sequence Output Types <!-- .element: class="r-fit-text" -->
  <img 
    src="https://miro.medium.com/1*QFOzE0TEMFERg3G5_5HiPA.png" 
    style="width: 99%; border-radius: 10px;">
  <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: ResearchGate DOI:10.1007/s11042-019-7885-5)
  </p>
  <div style="text-align: justify;">
    <p>
      RNNs can be configured for different input-output relationships:
    </p>
    <ul>
      <li><strong>One-to-one:</strong> fixed input → fixed output</li>
      <li><strong>One-to-many:</strong> fixed input → sequence output</li>
      <li><strong>Many-to-one:</strong> sequence input → single output</li>
      <li><strong>Many-to-many:</strong> sequence input → sequence output</li>
    </ul>
    <p>
      Each configuration reuses the same recurrent structure with shared weights across time steps.
    </p>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.8 - From RNNs to Transformers <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Traditional RNNs process sequences <strong>sequentially</strong>, making them slow and prone to vanishing gradients. <strong>Transformers</strong> replace recurrence with <strong>attention mechanisms</strong>, allowing <strong>parallel processing</strong> of all time steps and capturing <strong>long-range dependencies</strong> efficiently.
      </p>
      <p>
        Key innovations:
      </p>
      <ul>
        <li>No recurrence → full parallelization</li>
        <li>Attention → dynamic context weighting</li>
        <li>Positional encoding → preserves sequence order</li>
      </ul>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360634847/figure/fig1/AS:11431281143878155@1675754155978/The-network-architecture-for-an-RNN-and-Transformer.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.egyai.2023.100225)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.9 - Self-Attention Mechanism <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Self-attention computes relationships between all elements in a sequence. For input embeddings $X \in \mathbb{R}^{n \times d}$:
      </p>
      <p>
        $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
      </p>
      <p>
        The attention output is:
      </p>
      <p>
        $$\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$$
      </p>
      <p>
        This allows each token to <strong>attend</strong> to every other token in the sequence, weighted by relevance, capturing contextual relationships effectively.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360708722/figure/fig2/AS:1180034076278786@1658841632881/Illustration-of-the-self-attention-mechanism-Source-the-authors.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.jclepro.2022.133391)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.10 - Multi-Head Attention <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Multi-head attention</strong> extends self-attention by running several attention heads in parallel:
      </p>
      <p>
        $$\text{MHA}(Q,K,V) = \text{Concat}(head_1, \dots, head_h)W_O$$
      </p>
      <p>
        where each head is:
      </p>
      <p>
        $$head_i = \text{Attention}(QW_Q^{(i)}, KW_K^{(i)}, VW_V^{(i)})$$
      </p>
      <p>
        Each head captures different types of relationships (semantic, positional, syntactic). This is followed by a <strong>feedforward network</strong> applied to each position independently.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360708722/figure/fig3/AS:1180034076311555@1658841632929/Illustration-of-multi-head-attention-mechanism-Source-the-authors.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.jclepro.2022.133391)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.11 - Transformer Architecture <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        A standard <strong>Transformer</strong> block combines key components:
      </p>
      <ol>
        <li><strong>Multi-Head Self-Attention</strong> — captures token relationships</li>
        <li><strong>Add & Layer Normalization</strong> — stabilizes training</li>
        <li><strong>Position-wise Feedforward Network</strong> — processes each position</li>
        <li><strong>Residual Connections</strong> — facilitates gradient flow</li>
      </ol>
      <p>
        <strong>Encoder–Decoder structure:</strong> The encoder processes the input sequence, while the decoder generates the output sequence using masked attention. Transformers enable parallel computation and context-aware representations.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/356464739/figure/fig2/AS:1098942764888064@1639388181542/The-architecture-of-the-Transformer-model.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1038/s41598-021-04238-z)
      </p>
    </div>
  </div>
</section>