<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.2 - Recurrent Neural Networks

- [w2.2.1] Introduction to RNNs
- [w2.2.2] The Recurrent Connection
- [w2.2.3] Unfolding Through Time
- [w2.2.4] Vanishing and Exploding Gradients
- [w2.2.5] LSTM Networks
- [w2.2.6] GRU Networks
- [w2.2.7] Sequence Output Types
- [w2.2.8] Regularization and Optimization
- [w2.2.9] From RNNs to Transformers
- [w2.2.10] Self-Attention Mechanism
- [w2.2.11] Multi-Head Attention
- [w2.2.12] Transformer Architecture

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.1 - Recurrent Neural Networks <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Recurrent Neural Networks (RNNs)</strong> are designed to process <strong>sequential data</strong>, where the order of elements matters, such as time series, text, or audio signals. RNNs maintain a <strong>hidden state</strong> that captures information from previous time steps, allowing the network to model temporal dependencies.
      </p>
      <p>
        Common applications include language modeling, speech recognition, time-series forecasting, and sequence classification tasks.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/346263746/figure/fig4/AS:960987988992000@1606077945683/The-architecture-of-the-recurrent-neural-network-RNN-model.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/app10238585)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.2 - The Recurrent Connection <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        At each time step $t$, the RNN updates its hidden state using the current input $x_t$ and the previous hidden state $h_{t-1}$:
      </p>
      <p>
        $$h_t = \sigma_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$
      </p>
      <p>
        The output is computed as:
      </p>
      <p>
        $$y_t = \sigma_y(W_{hy} h_t + b_y)$$
      </p>
      <p>
        Where $x_t$ is the input at time $t$, $h_t$ is the hidden state, $y_t$ is the output, and $\sigma_h, \sigma_y$ are activation functions such as $\tanh$ or $\text{softmax}$.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://miro.medium.com/v2/resize:fit:1400/1*AQ52bwW55GsJt6HTxPDuMA.gif" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Medium - RNN recurrent connection visualization)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.3 - Unfolding Through Time <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        An RNN can be visualized as the same network <strong>unrolled through time</strong> for $T$ steps:
      </p>
      <p>
        $$h_t = f(h_{t-1}, x_t; \theta)$$
      </p>
      <p>
        This creates a computational graph where parameters $\theta = \{W_{xh}, W_{hh}, W_{hy}\}$ are <strong>shared across time</strong>. Each step passes information to the next, allowing temporal dependencies to propagate through the sequence.
      </p>
      <p>
        However, this structure leads to <strong>vanishing or exploding gradients</strong> during training, limiting the ability to learn long-term dependencies.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/322376746/figure/fig1/AS:614302073806865@1523474221312/The-standard-RNN-and-unfolded-RNN.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1186/s13040-017-0166-y)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.4 - Vanishing and Exploding Gradients <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        During <strong>backpropagation through time (BPTT)</strong>, gradients are multiplied across time steps:
      </p>
      <p>
        $$\frac{\partial L}{\partial W} = \sum_t \frac{\partial L_t}{\partial h_t} \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}}$$
      </p>
      <p>
        If eigenvalues of $\frac{\partial h_k}{\partial h_{k-1}}$ are less than 1, gradients vanish; if greater than 1, gradients explode. This limits the ability of standard RNNs to learn <strong>long-term dependencies</strong>.
      </p>
      <p>
        Solutions include gradient clipping for exploding gradients, and specialized architectures like LSTM and GRU for vanishing gradients.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/338135502/figure/fig2/AS:839399537451010@1577115527639/Illustration-of-vanishing-gradient-problem-in-RNN.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1109/ASYU48272.2019.8946437)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.5 - LSTM Networks <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Long Short-Term Memory (LSTM)</strong> networks mitigate gradient issues using <strong>gates</strong> to control information flow.
      </p>
      <p style="font-size: 0.85em;">
        $$\begin{aligned}
        f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
        i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
        \tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \\
        C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
        o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
        h_t &= o_t \odot \tanh(C_t)
        \end{aligned}$$
      </p>
      <p>
        LSTMs preserve long-term memory via the <strong>cell state</strong> $C_t$ and gated updates, enabling learning of dependencies across longer sequences.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/333709843/figure/fig13/AS:769733937229826@1560516242689/Visualization-of-Long-short-term-memory-LSTM-and-Gated-Recurrent-Unit-GRU.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1007/s11042-019-7885-5)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.6 - GRU Networks <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Gated Recurrent Units (GRUs)</strong> simplify the LSTM architecture by merging the forget and input gates into a single update gate.
      </p>
      <p>
        $$\begin{aligned}
        z_t &= \sigma(W_z [h_{t-1}, x_t]) \\
        r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
        \tilde{h}_t &= \tanh(W_h [r_t \odot h_{t-1}, x_t]) \\
        h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
        \end{aligned}$$
      </p>
      <p>
        GRUs are computationally faster and require fewer parameters while maintaining comparable performance to LSTMs, making them efficient for many sequence modeling tasks.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/333709843/figure/fig13/AS:769733937229826@1560516242689/Visualization-of-Long-short-term-memory-LSTM-and-Gated-Recurrent-Unit-GRU.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1007/s11042-019-7885-5)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.7 - Sequence Output Types <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        RNNs can be configured for different input-output relationships:
      </p>
      <ul>
        <li><strong>One-to-one:</strong> fixed input → fixed output (e.g., image classification)</li>
        <li><strong>One-to-many:</strong> fixed input → sequence output (e.g., image captioning)</li>
        <li><strong>Many-to-one:</strong> sequence input → single output (e.g., sentiment analysis)</li>
        <li><strong>Many-to-many:</strong> sequence input → sequence output (e.g., machine translation)</li>
      </ul>
      <p>
        Each configuration reuses the same recurrent structure with shared weights across time steps.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://miro.medium.com/v2/resize:fit:1400/1*fOF8uk1TfHk0eRZ97VF1tA.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: Medium - RNN sequence types)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.8 - Regularization and Optimization <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        To improve RNN training and generalization, modern approaches use several techniques:
      </p>
      <ul>
        <li><strong>Dropout:</strong> applied to non-recurrent connections to prevent overfitting</li>
        <li><strong>Layer Normalization:</strong> stabilizes hidden state dynamics</li>
        <li><strong>Gradient Clipping:</strong> prevents exploding gradients by capping gradient magnitude</li>
        <li><strong>Early Stopping:</strong> halts training when validation loss plateaus</li>
        <li><strong>Sequence Batching:</strong> groups sequences of similar length for computational efficiency</li>
      </ul>
      <p>
        These techniques ensure more stable and effective sequence learning.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/335845265/figure/fig2/AS:803743942262785@1568651155146/Illustration-of-the-dropout-mechanism-Dropout-randomly-drops-units-along-with-their.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.3390/s19194205)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.9 - From RNNs to Transformers <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Traditional RNNs process sequences <strong>sequentially</strong>, making them slow and prone to vanishing gradients. <strong>Transformers</strong> replace recurrence with <strong>attention mechanisms</strong>, allowing <strong>parallel processing</strong> of all time steps and capturing <strong>long-range dependencies</strong> efficiently.
      </p>
      <p>
        Key innovations:
      </p>
      <ul>
        <li>No recurrence → full parallelization</li>
        <li>Attention → dynamic context weighting</li>
        <li>Positional encoding → preserves sequence order</li>
      </ul>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360634847/figure/fig1/AS:11431281143878155@1675754155978/The-network-architecture-for-an-RNN-and-Transformer.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.egyai.2023.100225)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.10 - Self-Attention Mechanism <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        Self-attention computes relationships between all elements in a sequence. For input embeddings $X \in \mathbb{R}^{n \times d}$:
      </p>
      <p>
        $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
      </p>
      <p>
        The attention output is:
      </p>
      <p>
        $$\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$$
      </p>
      <p>
        This allows each token to <strong>attend</strong> to every other token in the sequence, weighted by relevance, capturing contextual relationships effectively.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360708722/figure/fig2/AS:1180034076278786@1658841632881/Illustration-of-the-self-attention-mechanism-Source-the-authors.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.jclepro.2022.133391)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.11 - Multi-Head Attention <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Multi-head attention</strong> extends self-attention by running several attention heads in parallel:
      </p>
      <p>
        $$\text{MHA}(Q,K,V) = \text{Concat}(head_1, \dots, head_h)W_O$$
      </p>
      <p>
        where each head is:
      </p>
      <p>
        $$head_i = \text{Attention}(QW_Q^{(i)}, KW_K^{(i)}, VW_V^{(i)})$$
      </p>
      <p>
        Each head captures different types of relationships (semantic, positional, syntactic). This is followed by a <strong>feedforward network</strong> applied to each position independently.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/360708722/figure/fig3/AS:1180034076311555@1658841632929/Illustration-of-multi-head-attention-mechanism-Source-the-authors.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1016/j.jclepro.2022.133391)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.12 - Transformer Architecture <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        A standard <strong>Transformer</strong> block combines key components:
      </p>
      <ol>
        <li><strong>Multi-Head Self-Attention</strong> — captures token relationships</li>
        <li><strong>Add & Layer Normalization</strong> — stabilizes training</li>
        <li><strong>Position-wise Feedforward Network</strong> — processes each position</li>
        <li><strong>Residual Connections</strong> — facilitates gradient flow</li>
      </ol>
      <p>
        <strong>Encoder–Decoder structure:</strong> The encoder processes the input sequence, while the decoder generates the output sequence using masked attention. Transformers enable parallel computation and context-aware representations.
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://www.researchgate.net/publication/356464739/figure/fig2/AS:1098942764888064@1639388181542/The-architecture-of-the-Transformer-model.png" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: ResearchGate DOI:10.1038/s41598-021-04238-z)
      </p>
    </div>
  </div>
</section>