<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w2.2 - Recurrent Neural Networks

- [w2.2.1] Introduction to RNNs
- [w2.2.2] The Recurrent Connection
- [w2.2.3] Unfolding Through Time
- [w2.2.4] Vanishing and Exploding Gradients
- [w2.2.5] LSTM Networks
- [w2.2.6] GRU Networks
- [w2.2.7] Sequence Output Types
- [w2.2.8] From RNNs to Transformers
- [w2.2.9] Self-Attention Mechanism
- [w2.2.10] Multi-Head Attention

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.1 - Recurrent Neural Networks <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Recurrent Neural Networks (RNNs)</strong> are designed to process <strong>sequential data</strong>, where the order of elements matters, such as time series, text, or audio signals. RNNs maintain a <strong>hidden state</strong> that captures information from previous time steps, allowing the network to model temporal dependencies.
  </p>
  <div style="flex: 0 0 50%; text-align: center;">
    <img 
      src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/unrolled-rnn_0.png" 
      style="width: 90%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
      (image source: https://builtin.com/data-science/recurrent-neural-networks-and-lstm)
    </p>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.2 - The Recurrent Connection <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        At each time step $t$, the RNN updates its hidden state using the current input $x_t$ and the previous hidden state $h_{t-1}$:
      </p>
      <p>
        $$h_t = \sigma_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$
      </p>
      <p>
        The output is computed as:
      </p>
      <p>
        $$y_t = \sigma_y(W_{hy} h_t + b_y)$$
      </p>
      <p>
        Where $x_t$ is the input at time $t$, $h_t$ is the hidden state, $y_t$ is the output, and $\sigma_h, \sigma_y$ are activation functions such as $\tanh$ or $\text{softmax}$.
      </p>
    </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.3 - Unfolding Through Time <!-- .element: class="r-fit-text" -->

  <p style="text-align: justify;">
    An RNN can be visualized as the same network <strong>unrolled through time</strong> for $T$ steps:
  </p>
  <img 
    src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/unrolled-rnn_0.png" 
    style="width: 70%; border-radius: 10px;">
  <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: https://builtin.com/data-science/recurrent-neural-networks-and-lstm)
  </p>
  <p style="text-align: justify;">
    $$h_t = f(h_{t-1}, x_t; \theta)$$
  </p>
  <p style="text-align: justify;">
    This creates a computational graph where parameters $\theta = \{W_{xh}, W_{hh}, W_{hy}\}$ are <strong>shared across time</strong>. Each step passes information to the next, allowing temporal dependencies to propagate through the sequence.
  </p>
  <p style="text-align: justify;">
    However, this structure leads to <strong>vanishing or exploding gradients</strong> during training, limiting the ability to learn long-term dependencies.
  </p>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.4 - Vanishing and Exploding Gradients <!-- .element: class="r-fit-text" -->

  <div style="text-align: justify;">
    <h3 class="r-fit-text"></h3>
    <p>
      During <strong>backpropagation through time (BPTT)</strong>, gradients are multiplied across time steps:
    </p>
    <p>
      $$\frac{\partial L}{\partial W} = \sum_t \frac{\partial L_t}{\partial h_t} \prod_{k=1}^{t} \frac{\partial h_k}{\partial h_{k-1}}$$
    </p>
    <p>
      If eigenvalues of $\frac{\partial h_k}{\partial h_{k-1}}$ are less than 1, gradients vanish; if greater than 1, gradients explode. This limits the ability of standard RNNs to learn <strong>long-term dependencies</strong>.
    </p>
    <p>
      Solutions include gradient clipping for exploding gradients, and specialized architectures like LSTM and GRU for vanishing gradients.
    </p>
  </div>
  
  </section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.5 - LSTM Networks (1) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Long Short-Term Memory (LSTM)</strong> networks mitigate gradient issues using <strong>gates</strong> to control information flow.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
        $$\begin{aligned}
        f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
        i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
        \tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \\
        C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
        o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
        h_t &= o_t \odot \tanh(C_t)
        \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://thorirmar.com/post/insight_into_lstm/featured.png" 
        style="width: 99%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://thorirmar.com/post/insight_into_lstm/)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    LSTMs preserve long-term memory via the <strong>cell state</strong> $C_t$ and gated updates, enabling learning of dependencies across longer sequences.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.5 - LSTM Networks (2) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Long Short-Term Memory (LSTM)</strong> networks mitigate gradient issues using <strong>gates</strong> to control information flow.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
      $$\begin{aligned}
      \text{Forget gate: } & f_t \\
      \text{Input gate: } & i_t \\
      \text{Candidate cell state: } & \tilde{C}_t  \\
      \text{Cell state: } & C_t \\
      \text{Output gate: } & o_t \\
      \text{Hidden state: } & h_t
      \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://thorirmar.com/post/insight_into_lstm/featured.png" 
        style="width: 99%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://thorirmar.com/post/insight_into_lstm/)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    $f_t$ decides what to forget from the previous memory, $i_t$ controls the ne information to store, $\tilde{C}$ new candidate content for memory, $C_t$ updated long term memory of LSTM, $o_t$ controls what part of the cell state is output, $h_t$ output of LSTM for current timestep.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.6 - GRU Networks (1) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Gated Recurrent Units (GRUs)</strong> simplify the LSTM architecture by merging the forget and input gates into a single update gate.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        $$\begin{aligned}
        z_t &= \sigma(W_z [h_{t-1}, x_t]) \\
        r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
        \tilde{h}_t &= \tanh(W_h [r_t \odot h_{t-1}, x_t]) \\
        h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
        \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnNORKUR4B90lrIt8zoVCoVRkyGXtyCbtcZw&s" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: DOI:10.13140/RG.2.2.15743.46240)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    GRUs are computationally faster and require fewer parameters while maintaining comparable performance to LSTMs, making them efficient for many sequence modeling tasks.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.6 - GRU Networks (2) <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    <strong>Gated Recurrent Units (GRUs)</strong> simplify the LSTM architecture by merging the forget and input gates into a single update gate.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 50%; max-width: 50%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p style="font-size: 0.85em;">
      $$\begin{aligned}
      \text{Update gate: } & z_t \\
      \text{Reset gate: } & r_t \\
      \text{Candidate hidden state: } & \tilde{h}_t \\
      \text{Hidden state: } & h_t
      \end{aligned}$$
      </p>
    </div>
    <div style="flex: 0 0 50%; text-align: center;">
      <img 
        src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnNORKUR4B90lrIt8zoVCoVRkyGXtyCbtcZw&s" 
        style="width: 90%; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: DOI:10.13140/RG.2.2.15743.46240)
      </p>
    </div>
  </div>
  <p style="text-align: justify;">
    $z_t$ controls how much of the previous hidden steate to keep, $r_t$ determines how to combine new input with previous memory, $\tilde{h}$ new candidate content for hidden state, $h_t$ final hidden state combining old and new information.
  </p>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.7 - Sequence Output Types <!-- .element: class="r-fit-text" -->
  <img 
    src="https://miro.medium.com/1*QFOzE0TEMFERg3G5_5HiPA.png" 
    style="width: 99%; border-radius: 10px;">
  <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: ResearchGate DOI:10.1007/s11042-019-7885-5)
  </p>
  <div style="text-align: justify;">
    <p>
      RNNs can be configured for different input-output relationships:
    </p>
    <ul>
      <li><strong>One-to-one:</strong> fixed input → fixed output</li>
      <li><strong>One-to-many:</strong> fixed input → sequence output</li>
      <li><strong>Many-to-one:</strong> sequence input → single output</li>
      <li><strong>Many-to-many:</strong> sequence input → sequence output</li>
    </ul>
    <p>
      Each configuration reuses the same recurrent structure with shared weights across time steps.
    </p>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.8 - From RNNs to Transformers <!-- .element: class="r-fit-text" -->
  <p style="text-align: justify;">
    Traditional RNNs process sequences <strong>token-by-token</strong>, creating bottlenecks and suffering from vanishing gradients over long sequences. <strong>Transformers</strong> eliminate recurrence entirely, using <strong>self-attention</strong> to process all tokens <strong>in parallel</strong> while capturing dependencies regardless of distance.
  </p>
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 5rem;
  ">
    <div style="flex: 0 0 60%; max-width: 60%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Key advantages:</strong>
      </p>
      <ul>
        <li><strong>Parallelization:</strong> All positions processed simultaneously</li>
        <li><strong>Long-range dependencies:</strong> Direct connections between any two tokens</li>
        <li><strong>Scalability:</strong> Efficient training on modern hardware (GPUs/TPUs)</li>
        <li><strong>Positional encoding:</strong> Injects sequence order information</li>
      </ul>
    </div>
    <div style="flex: 0 0 40%; text-align: center;">
      <img 
        src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" 
        style="width: 90%; border-radius: 10px;"
        alt="RNN vs Transformer architecture comparison">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://machinelearningmastery.com/)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.9 - Self-Attention Mechanism <!-- .element: class="r-fit-text" -->

  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 16rem;
  ">
    <div style="flex: 0 0 55%; max-width: 55%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
        <strong>Self-attention</strong> computes relevance-weighted representations by comparing each token with all others. Given input embeddings $X \in \mathbb{R}^{n \times d}$, we project into three spaces:
      </p>
      <p>
        $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
      </p>
      <p>
        The attention mechanism computes:
      </p>
      <p>
        $$\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$$
      </p>
      <p>
        <strong>Intuition:</strong> Queries ask "what am I looking for?", Keys answer "what do I contain?", and Values provide "what information to pass forward". The scaling factor $\sqrt{d_k}$ prevents softmax saturation in high dimensions.
      </p>
    </div>
    <div style="flex: 0 0 45%; text-align: center;">
      <img 
        src="https://miro.medium.com/1*eUYWoxniuhP8h2LMTZg2yA.png" 
        style="width: 100%; object-fit: contain; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://medium.com/gojekengineering/how-transformers-understand-language-attention-explained-simply-5ec89c54ae9d)
      </p>
    </div>
  </div>
</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w2.2.10 - Multi-Head Attention <!-- .element: class="r-fit-text" -->
  <div style="
    display: flex; 
    align-items: center; 
    justify-content: center; 
    gap: 16rem;
  ">
    <div style="flex: 0 0 55%; max-width: 55%;text-align: justify;">
      <h3 class="r-fit-text"></h3>
      <p>
      <strong>Multi-head attention</strong> runs $h$ parallel attention mechanisms, each learning different relationship patterns:
    </p>
    <p>
      $$\text{MHA}(Q,K,V) = \text{Concat}(head_1, \dots, head_h)W_O$$
    </p>
    <p>
      where each head computes:
    </p>
    <p>
      $$head_i = \text{Attention}(QW_Q^{(i)}, KW_K^{(i)}, VW_V^{(i)})$$
    </p>
    <p>
      <strong>Why multiple heads?</strong> Different heads specialize in different aspects: syntactic structure, semantic relationships, positional patterns, etc. The concatenated output is projected through $W_O$ to combine these diverse representations
    </div>
    <div style="flex: 0 0 45%; text-align: center;">
      <img 
        src="https://miro.medium.com/1*eUYWoxniuhP8h2LMTZg2yA.png" 
        style="width: 100%; object-fit: contain; border-radius: 10px;">
      <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
        (image source: https://medium.com/gojekengineering/how-transformers-understand-language-attention-explained-simply-5ec89c54ae9d)
      </p>
    </div>
  </div>
</section>