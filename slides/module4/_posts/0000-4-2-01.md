<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w14.1 Quantum Kernel Methods (Theory) <!-- .element: class="r-fit-text" -->

- [w14.1.1] What are kernel methods?
- [w14.1.2] Kernel Trick
- [w14.1.3] Kernel Definition
- [w14.1.4] Introduction to the demo

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.1 - What are kernle methods? (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

To understand what a kernel method does, let’s first revisit one of the simplest methods to assign binary labels to datapoints: linear classification.

Imagine we want to discern two different classes of points that lie in different corners of the plane. A linear classifier corresponds to drawing a line and assigning different labels to the regions on opposing sides of the line:

<div style="text-align: center;">
    <img 
    src="https://blog-assets.cloud.pennylane.ai/demos/tutorial_kernels_module/main/_assets/images/linear_classification.png" 
    style="width: 90%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: https://pennylane.ai/qml/demos/tutorial_kernels_module)
    </p>
</div>

</div>
</section>
<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.1 - What are kernle methods? (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

We can mathematically formalize this by assigning the label $y$ via

$$y(x)=sgn(\langle\omega,x\rangle+b)$$

The vector $\omega$ points perpendicular to the line and thus determine its slope. The independent term $b$ specifies the position on the plane. In this form, linear classification can also be extended to higher dimensional vectors $x$ where a line does not divide the entire space into two regions anymore. Instead one needs a hyperplane.
</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.1 - What are kernle methods? (3) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

It is immediately clear that this method is not very powerful, as datasets that are not separable by a hyperplane can’t be classified without error.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.2 - Kernel Trick (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

We can actually sneak around this limitation by performing a neat trick: if we define some map $\phi(x)$ that embeds our datapoints into a larger feature space and then perform linear classification there, we could actually realise non-linear classification in our original space.

<div style="text-align: center;">
    <img 
    src="https://blog-assets.cloud.pennylane.ai/demos/tutorial_kernels_module/main/_assets/images/embedding_nonlinear_classification.png?w=828" 
    style="width: 90%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    (image source: https://pennylane.ai/qml/demos/tutorial_kernels_module)
    </p>
</div>

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.2 - Kernel Trick (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

If we go back to the expression for our prediction and include the embedding, we get

$$y(x)=sgn(\langle\omega,\phi(x)\rangle+b)$$

</div>

We will forgo one tiny step, but it can be shown that for the purpose of optimal classification, we can choose the vector defining the decision boundary as a linear combination of the embedded datapoints 
$\omega=\sum_i{\alpha_i\phi(x_i)}$.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.2 - Kernel Trick (3) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Putting this into the formula yields

$$y(x) = sng\left(\sum_i\alpha_i\langle\phi(x_i),\phi(x)\rangle+b\right)$$

This rewriting might not seem useful at first, but notice the above formula only contains inner products between vectors in the embedding space:

$$k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.3 - Kernel Definition (1)<!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

$$k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$$

We call this function the kernel. It provides the advantage that we can often find an explicit formula for the kernel $k$ that makes it superfluous to actually perform the (potentially expensive) embedding $\phi$.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w14.1.3 - Kernel Definition (2)<!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Consider for example the following embedding and the associated kernel:

$$\phi((x_1,x_2))=(x_1^2,\sqrt(2)x_1x_2,x_2^2)$$
$$k(x,y)=x_1^2x_2^2+2x_1x_2y_1y_2+x^2_2y^2_2=\langle x,y\rangle^2$$

This means by just replacing the regular scalar product in our linear classification with the map 
$k$ we can actually express much more intricate decision boundaries.

This is very important, because in many interesting cases the embedding $\phi$ will be much costlier to compute than the kernel $k$.

</section>

### w14.1.4 - Introduction to the demo<!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

In the following demo, we will explore one particular kind of kernel that can be realized on near-term quantum computers, namely Quantum Embedding Kernels (QEKs). 

These are kernels that arise from embedding data into the space of quantum states. We formalize this by considering a parameterised quantum circuit $U(x)$ that maps a datapoint $x$ to the state

$$|\Psi(x)\rangle=U(x)|0\rangle$$

The kernel value is then given by the overlap of the associated embedded quantum states

$$k(x_i, x_j)=|\langle\psi(x_i)|\psi(x_j)\rangle|^2$$

</section>