<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w5 - Variational Circuits <!-- .element: class="r-fit-text" -->

- [w5.1] How to interpret a quantum circuit as a model
- [w5.1.1] Deterministic quantum models
- [w5.1.2] Probabilistic quantum models
- [w5.1.3] Example 1 - VQC
- [w5.1.4] Example 2 - VG
- [w5.2] Which functions do VQMs express?
- [w5.3] Training VQMs
- [w5.3.1] Gradients of quantum computations + parametric shift rule
- [w5.3.3] Barren plateau
- [w5.3.4] Generative training
- [w5.4] Quantum circuit and NNs
- [w5.4.1] Emulating non linear activations via encoding
- [w5.4.2] VCs as DNNs

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1 - How to interpret a quantum circuit as a model <!-- .element: class="r-fit-text" -->
  
<img src="{{asset_folder}}/qml_pipeline.png" style="width: 90%; border-radius: 10px;">

In machine learning, a model is a function f_Î¸: X â†’ Y or probability distribution p_Î¸.

Variational circuits can be interpreted as ML models by applying circuit U(x, Î¸) depending on input x and parameters Î¸, then interpreting measurement outcomes as model outputs

Two types: Deterministic (expectation values) and Probabilistic (distributions)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.1 - Deterministic quantum models (1) <!-- .element: class="r-fit-text" -->

Definition 5.1 (Deterministic quantum model)

Let X be a data input domain, U(x, Î¸) a quantum circuit depending on inputs and parameters, and M a Hermitian operator. 

The function

f_Î¸(x) = âŸ¨Ïˆ(x, Î¸)|M|Ïˆ(x, Î¸)âŸ©

or in density matrix notation,

f_Î¸(x) = tr{MÏ(x, Î¸)}

with Ï(x, Î¸) = U(x, Î¸)â€ |0âŸ©âŸ¨0|U(x, Î¸) defines a deterministic variational quantum model.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.1 - Deterministic quantum models (2) <!-- .element: class="r-fit-text" -->

Circuit Structure:

Popular choice: data embedding S(x) + parametrised block W(Î¸)

U(x, Î¸) = W(Î¸)S(x)

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/circuit_structure.png" 
    style="width: 75%; border-radius: 10px;">
</div>

Both blocks decompose into gates e^(-iaG) (time-evolution encoding) and fixed unitaries T_i, V_k

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.2 - Probabilistic quantum models (1) <!-- .element: class="r-fit-text" -->

Definition 5.2 (Supervised probabilistic quantum model)

Associate measurement eigenvalues with outputs y âˆˆ Y:

p_Î¸(y|x) = |âŸ¨y|Ïˆ(x, Î¸)âŸ©|Â²

where M = Î£_{yâˆˆY} y|yâŸ©âŸ¨y|

Example: Binary classification with Pauli-Z measurement M = |0âŸ©âŸ¨0| - |1âŸ©âŸ¨1|
- y = -1 corresponds to |0âŸ© 
- y = 1 corresponds to |1âŸ©

Model naturally samples from p_Î¸(y|x)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.2 - Probabilistic quantum models (2) <!-- .element: class="r-fit-text" -->

Definition 5.3 (Unsupervised probabilistic quantum model)

Associate measurement outcomes with data points x âˆˆ X:

p_Î¸(x) = |âŸ¨x|Ïˆ(Î¸)âŸ©|Â²

where |Ïˆ(Î¸)âŸ© = W(Î¸)|0âŸ© and M = Î£_x x|xâŸ©âŸ¨x|

Also known as Born machines (from Born rule)

Naturally generative: implementation produces samples not probabilities

Can be hard to estimate p_Î¸(x) explicitly (exponential in qubits)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.3 - Example 1 - VQC (1) <!-- .element: class="r-fit-text" -->

Simple single-qubit variational quantum classifier

Scalar input x âˆˆ â„

Circuit:  |0âŸ© â†’ R_x(x) â†’ Rot(Î¸â‚, Î¸â‚‚, Î¸â‚ƒ) â†’ âŸ¨Ïƒ_zâŸ©

Components:
- R_x(x) = exp(-ix/2 Ïƒ_x): Pauli-X rotation (data encoding)
- Rot(Î¸â‚, Î¸â‚‚, Î¸â‚ƒ) = R_z(Î¸â‚)R_y(Î¸â‚‚)R_z(Î¸â‚ƒ): general single-qubit rotation (trainable)
- Ïƒ_z: Pauli-Z measurement

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.3 - Example 1 - VQC (2) <!-- .element: class="r-fit-text" -->

Explicit computation of model function:

Data encoding: 
|Ï†(x)âŸ© = R_x(x)|0âŸ© = [cos(x/2), -iÂ·sin(x/2)]^T

After applying Rot(Î¸â‚, Î¸â‚‚, Î¸â‚ƒ) and measuring Ïƒ_z:

f_Î¸(x) = cos(Î¸â‚‚)cos(x) - sin(Î¸â‚)sin(Î¸â‚‚)sin(x)

Properties:
- Output in [-1, 1] (eigenvalues of Ïƒ_z)
- Trigonometric/periodic structure
- Only depends on Î¸â‚, Î¸â‚‚ (not Î¸â‚ƒ)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.3 - Example 1 - VQC (3) <!-- .element: class="r-fit-text" -->

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/vqc_plots.png" 
    style="width: 90%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    Left: f_Î¸(x) vs x for fixed Î¸â‚=Î¸â‚‚=1.6. Right: f_Î¸(x) vs (Î¸â‚, Î¸â‚‚) for fixed x=1.6
    </p>
</div>

Further processing:

Binary classifier: y = 1 if f_Î¸(x) > 0, else y = -1

Probability: p(1) = (f_Î¸(x) + 1)/2,  p(0) = 1 - p(1)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.4 - Example 2 - VG (1) <!-- .element: class="r-fit-text" -->

Variational Generator: Bars-and-Stripes Dataset

<div style="
    display: flex; 
    align-items: center;
    justify-content: center; 
    gap: 3rem;
  ">
    <div style="flex: 0 0 55%; max-width: 55%;text-align: justify;">
      <p>
        <strong>Task:</strong> Generate 2Ã—2 black-and-white images
      </p>
      <p>
        <strong>Setup:</strong>
        <ul>
            <li> 4 "visible" qubits encode pixels (basis encoding)</li>
            <p></p>
            <li> 3 "hidden" qubits (unmeasured) increase capacity</li>
            <p></p>
            <li> Example: (w,w,b,b) â†’ |0011âŸ©</li>
        </ul>
      </p>
    </div>
    <div style="flex: 0 0 45%; text-align: center;">
      <img 
        src="{{asset_folder}}/bars_stripes.png" 
        style="width: 90%; border-radius: 10px;">
    </div>
  </div>

Circuit: |Ïˆ(Î¸)âŸ© = W(Î¸)|0000000âŸ© then measure first 4 qubits with Pauli-Z

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.1.4 - Example 2 - VG (2) <!-- .element: class="r-fit-text" -->

Probabilistic generative model:

p_Î¸(x) = |âŸ¨x|Ïˆ(Î¸)âŸ©|Â²,  x âˆˆ {0,1}âŠ—â´

Optimal state for uniform bars-and-stripes distribution:

|Ïˆ_optâŸ© = 1/2 (|1010âŸ© + |0101âŸ© + |1100âŸ© + |0011âŸ©)

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/optimal_bars.png" 
    style="width: 60%; border-radius: 10px;">
</div>

Hidden qubits provide additional degrees of freedom for W(Î¸)

Inspired by Restricted Boltzmann Machines: restrict W to entangle only hidden-visible pairs

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.2 - Which functions do VQMs express? (1) <!-- .element: class="r-fit-text" -->

Key insight: Quantum models with time-evolution encoding are linear combinations of trigonometric functions

Theorem 5.1 (Function class of quantum models)

For encoding S_i(x_i) = exp(-ix_i G_i) with diagonal generator G_i = diag(Î»â‚â±,...,Î»_dâ±):

f_Î¸(x) = Î£_{Ï‰â‚âˆˆÎ©â‚} Â·Â·Â· Î£_{Ï‰_NâˆˆÎ©_N} c_{Ï‰â‚...Ï‰_N}(Î¸) exp(iÏ‰â‚xâ‚) Â·Â·Â· exp(iÏ‰_Nx_N)

Frequency spectrum: Î©_i = {Î»_sâ± - Î»_tâ± | s,t âˆˆ {1,...,d}}

Properties: 0 âˆˆ Î©, and Ï‰ âˆˆ Î© â‡’ -Ï‰ âˆˆ Î© (guarantees real output)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.2 - Which functions do VQMs express? (2) <!-- .element: class="r-fit-text" -->

Corollary 5.1 (Fourier series for Pauli rotations)

If eigenvalue differences are integer-valued (e.g., Pauli rotations G = Ïƒ/2):

f_Î¸(x) = Î£_{nâ‚âˆˆÎ©â‚} Â·Â·Â· Î£_{n_NâˆˆÎ©_N} c_{nâ‚...n_N}(Î¸) exp(inâ‚xâ‚) Â·Â·Â· exp(in_Nx_N)

where n_i âˆˆ â„¤ - Multi-dimensional Fourier series!

Can rewrite with sine/cosine: exp(inx) = cos(nx) + iÂ·sin(nx)

Since output real: c_Ï‰ = c*_{-Ï‰}

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.2 - Which functions do VQMs express? (3) <!-- .element: class="r-fit-text" -->

Example: Pauli-rotation encoding

Single encoding: Î© = {-1, 0, 1}

f_Î¸(x) = c_{-1}exp(-ix) + câ‚€ + câ‚exp(ix) = câ‚€ + 2Re(câ‚)cos(x) + 2Im(câ‚)sin(x)

Repeated L times: Î© = {-L,...,0,...,L}

f_Î¸(x) = Î£_{n=-L}^L c_n(Î¸) exp(inx)

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/repeated_encoding.png" 
    style="width: 75%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    Repeating encoding increases available frequencies
    </p>
</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.2 - Which functions do VQMs express? (4) <!-- .element: class="r-fit-text" -->

Key implications:

1. Periodicity: Models are 2Ï€-periodic in each feature - Cannot distinguish x_i from x_i + 2Ï€ - Need careful data preprocessing/scaling

2. Limited frequencies: Expressivity controlled by encoding Hamiltonians G_i

3. Coefficients matter: Variational circuit must be expressive enough to realize all c_Ï‰(Î¸)

4. Repeated encoding enriches spectrum: Increases available frequencies

5. Universal approximation: With sufficient repetitions + circuit depth â†’ can approximate any function (via Fourier series universality)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3 - Training VQMs <!-- .element: class="r-fit-text" -->

Hybrid quantum-classical training approach:

<div style="
    display: flex; 
    align-items: center;
    justify-content: center; 
    gap: 3rem;
  ">
    <div style="flex: 0 0 55%; max-width: 55%;text-align: justify;">
      <p>
        <strong>Loop:</strong>
        <ul>
            <li> Quantum: Evaluate f_Î¸(x) = âŸ¨MâŸ©_{x,Î¸}</li>
            <p></p>
            <li> Classical: Compute cost C(Î¸) and gradients âˆ‡_Î¸ C</li>
            <p></p>
            <li> Update: Î¸ â† Î¸ - Î·âˆ‡_Î¸ C</li>
        </ul>
        <p></p>
        <br/>
        <p>
        <strong>Challenge:</strong> How to compute âˆ‡_Î¸ C when model evaluated on quantum hardware?
        </p>
      </p>
    </div>
    <div style="flex: 0 0 45%; text-align: center;">
      <img 
        src="{{asset_folder}}/hybrid_training.png" 
        style="width: 90%; border-radius: 10px;">
    </div>
  </div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.1 - Gradients of quantum computations (1) <!-- .element: class="r-fit-text" -->

Problem with finite differences:

âˆ‚f_Î¸/âˆ‚Î¼ â‰ˆ (f_Î¸ - f_{Î¸+Î”Î¸})/Î”Î¼

- Small gradients need small Î”Î¼ â†’ high precision required
- Precision Îµ requires O(Îµâ»Â²) measurements
- Not practical for noisy quantum hardware

Issue with analytical derivatives:

âˆ‚/âˆ‚Î¼ âŸ¨Ïˆ|Gâ€ (Î¼)BG(Î¼)|ÏˆâŸ© = âŸ¨Ïˆ|Gâ€ B(âˆ‚G/âˆ‚Î¼)|ÏˆâŸ© + âŸ¨Ïˆ|(âˆ‚Gâ€ /âˆ‚Î¼)BG|ÏˆâŸ©

Not a quantum expectation (bra â‰  ket)!

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.1 - Parameter-shift rules (1) <!-- .element: class="r-fit-text" -->

Definition 5.4 (Parameter-shift rule)

Express derivative as linear combination of shifted expectations:

âˆ‚f_Î¼/âˆ‚Î¼ = Î£_i a_i f_{Î¼+s_i}

where {a_i}, {s_i} are real scalars

For gates G(Î¼) = exp(-iÎ¼G) with GÂ² = 1 (e.g., Pauli rotations):

âˆ‚f_Î¼/âˆ‚Î¼ = [f_{Î¼+s} - f_{Î¼-s}] / (2sin(s))

Typically choose s = Ï€/2 so denominator = 1/2

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.1 - Parameter-shift rules (2) <!-- .element: class="r-fit-text" -->

Derivation sketch for GÂ² = 1:

Gate: G(Î¼) = exp(-iÎ¼G) = cos(Î¼)ğŸ™ - iÂ·sin(Î¼)G

Conjugated measurement: K(Î¼) = Gâ€ (Î¼)BG(Î¼) = A + BÂ·cos(Î¼) + CÂ·sin(Î¼)

Using trigonometric identities for derivatives:
âˆ‚cos(Î¼)/âˆ‚Î¼ = [cos(Î¼+s) - cos(Î¼-s)] / [2sin(s)]

Leads to the parameter-shift rule.

Advantages:
- Exact gradient (not approximation)
- Only 2 circuit evaluations per parameter
- Same cost as forward pass

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.1 - Parameter-shift rules (3) <!-- .element: class="r-fit-text" -->

Practical considerations:

Scaling: 2K Ã— S circuit evaluations
- K: number of parameters
- S: shots per expectation estimate
- Can use low S early in training (unbiased estimator)

Comparison to classical:
- Backpropagation: roughly 1 evaluation for full gradient
- Parameter-shift: 2K evaluations
- Scaling disadvantage, but no-cloning theorem prevents better

Extensions:
- Works for parameters in multiple gates (use product rule)
- General gates: stochastic or modified shift rules
- Can compute Hessians, Fubini-Study metric tensor

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.3 - Barren plateaus (1) <!-- .element: class="r-fit-text" -->

Problem: Gradients vanish exponentially with system size

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/barren_plateaus.png" 
    style="width: 80%; border-radius: 10px;">
    <p style="font-size: 0.3em; color: #888; margin-top: 0.5em;">
    Optimization landscape becomes exponentially flat
    </p>
</div>

Definition: Regions in parameter space where ||âˆ‡_Î¸ f_Î¸|| â‰ˆ 0 with high probability

Consequence: 
- Optimization slow/expensive
- Need high precision to resolve small gradients
- O(Îµâ»Â²) measurements for precision Îµ

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.3 - Barren plateaus (2) <!-- .element: class="r-fit-text" -->

Theorem 5.2 (McClean et al.) 

For f_Î¼ = âŸ¨0|W(Î¼)â€ MW(Î¼)|0âŸ© with W(Î¼) = AG(Î¼)B:

If A or B (or both) sampled from 2-design (e.g., Haar random):

Var[âˆ‚_Î¼ f_Î¼] = O(2â»â¿)

where n = number of qubits

Occurs when:
- Circuits sufficiently deep
- Global measurements
- High noise levels
- High entanglement entropy

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.3 - Barren plateaus (3) <!-- .element: class="r-fit-text" -->

Key insight: Large Hilbert spaces + random circuits â†’ vanishing gradients

Intuition: 
- Moving in exponentially large space
- Single parameter has negligible effect on average
- Like random walk in high dimensions

Mitigation strategies:
1. Use structured ansÃ¤tze (not random)
2. Restrict to relevant subspaces
3. Layer-wise training
4. Choose encodings wisely
5. Use problem-specific structure

Not a fundamental limitation but a circuit design challenge!

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.4 - Generative training (1) <!-- .element: class="r-fit-text" -->

Generative Adversarial Networks (GANs) for quantum models:

<div style="
    display: flex; 
    align-items: center;
    justify-content: center; 
    gap: 3rem;
  ">
    <div style="flex: 0 0 55%; max-width: 55%;text-align: justify;">
      <p>
        <strong>Setup:</strong>
        <ul>
            <li> <strong>Generator:</strong> Variational circuit G(Î¸_G) producing samples</li>
            <p></p>
            <li> <strong>Discriminator:</strong> Model D(Î¸_D) distinguishing real from fake</li>
        </ul>
        <p></p>
        <br/>
        <p>
        <strong>Training:</strong>
        <br/>
        - D: Minimize C_D = p(1|G) - p(1|real)
        <br/>
        - G: Minimize C_G = -p(1|G)
        </p>
      </p>
    </div>
    <div style="flex: 0 0 45%; text-align: center;">
      <img 
        src="{{asset_folder}}/qgan.png" 
        style="width: 90%; border-radius: 10px;">
    </div>
  </div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.3.4 - Generative training (2) <!-- .element: class="r-fit-text" -->

Quantum twist: If true data from quantum circuit, skip measurement!

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/quantum_gan.png" 
    style="width: 70%; border-radius: 10px;">
</div>

Run discriminator D directly on quantum state:
- Either from generator G(Î¸_G)|0âŸ©
- Or from "real" circuit C|0âŸ©

Measure designated qubit: outcome âˆˆ {-1, 1} indicates prediction

Applications: Circuit approximation, state learning

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4 - Quantum circuits and Neural Networks (1) <!-- .element: class="r-fit-text" -->

Are variational circuits "quantum neural networks"?

Misleading because:
- NNs = multi-layer perceptrons with nonlinear activations
- Quantum circuits don't naturally have this structure
- Quantum evolution fundamentally linear (unitary)

However, connections exist:
1. Can emulate nonlinear activations (encoding-dependent)
2. Can interpret as deep linear NNs
3. Time-evolution encoding â†” exponential activation

Term persists due to:
- Training similarities (gradient descent)
- Layered structure
- Parameter optimization

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.1 - Emulating non-linear activations (1) <!-- .element: class="r-fit-text" -->

Challenge: Implement nonlinear Ï†: v â†¦ Ï†(v)

Depends on encoding:

Amplitude encoding:
- Nonlinear transformation of amplitudes impossible with unitaries
- Need measurements + branch selection
- Costly to switch representations

Basis encoding:
- Transform |vâŸ©|0âŸ© â†’ |vâŸ©|Ï†(v)âŸ© easier
- Can use classical algorithms â†’ reversible â†’ quantum

Rotation encoding:
- Repeat-until-success circuits
- Non-deterministic runtime

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.1 - Emulating non-linear activations (2) <!-- .element: class="r-fit-text" -->

Basis encoding examples:

Step function: Ï†(v) = 0 if v â‰¤ 0, else 1
- Just read sign bit!

ReLU: Ï†(v) = 0 if v â‰¤ 0, else v
- Conditional copy based on sign bit

Sigmoid: Ï†(v) = 1/(1+exp(-Î´v))
- Piecewise linear approximation
- Boolean function lookup (Quine-McCluskey)
- 6 fractional bits â†’ smooth approximation

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.1 - Emulating non-linear activations (3) <!-- .element: class="r-fit-text" -->

Rotation encoding: repeat-until-success

For sigmoid-like Ï†(v) = arctan(tanÂ²(v)):

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/repeat_until_success.png" 
    style="width: 70%; border-radius: 10px;">
</div>

1. Encode v in angle of "net input qubit"
2. Apply controlled operations with output qubit
3. Measure ancilla
   - Success (|0âŸ©): output rotated by Ï†(v)
   - Failure (|1âŸ©): reset and repeat

Non-deterministic but achieves nonlinearity

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.2 - VCs as DNNs (1) <!-- .element: class="r-fit-text" -->

Interpret quantum circuit as neural network:

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/circuit_as_nn.png" 
    style="width: 80%; border-radius: 10px;">
</div>

- Input layer: Amplitudes âŸ¨Î¼_i|Ï†(x)âŸ© in measurement eigenbasis
- Hidden layers: Gates = linear (unitary) transformations
  - Trainable gates â†’ weight matrices W(Î¸)
  - Fixed gates â†’ fixed matrices
- Output layer: Measurement = nonlinear layer
  - Activation: Ï†(a) = |a|Â²
  - Linear combination with weights Î¼_i

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.2 - VCs as DNNs (2) <!-- .element: class="r-fit-text" -->

Structure: Deep linear neural network (constant width)

Single-qubit gate connectivity:

For n qubits, gate on qubit i:
- Sparse 2â¿ Ã— 2â¿ matrix
- Connects 4 sets of 2 variables with same weights (parameter tying)
- Controlled gates: removes half the ties, adds identities

<div style="text-align: center;">
    <img 
    src="{{asset_folder}}/gate_connectivity.png" 
    style="width: 65%; border-radius: 10px;">
</div>

Use: Links to linear NN theory (training dynamics, expressivity)

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w5.4.2 - VCs as DNNs (3) <!-- .element: class="r-fit-text" -->

Alternative: Time-evolution as exponential activation

From Fourier formalism:

f_Î¸(x) = Î£_{Ï‰âˆˆÎ©} c_Ï‰(Î¸) exp(iÏ‰Â·x)

Can write as single-hidden-layer NN:

f_{w,W}(x) = wÂ·Ï†(Wx) = Î£_j w_j exp(iW_jÂ·x)

- Hidden activation: Ï†(a) = exp(ia) (complex exponential)
- Input weights W: Fixed frequency vectors Ï‰ (from encoding)
- Output weights w: Fourier coefficients c_Ï‰(Î¸) (trainable)

Connection: Links to Random Fourier Features and kernel methods

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### Summary <!-- .element: class="r-fit-text" -->

Key takeaways:

1. Variational circuits = hybrid quantum-classical models for NISQ devices

2. Two types: Deterministic (expectations) | Probabilistic (Born machines)

3. Expressivity: Fourier series, limited by encoding + ansatz

4. Training: Parameter-shift rules enable exact gradients on quantum hardware

5. Barren plateaus: Vanishing gradients challenge, not fundamental limitation

6. vs Neural Networks: Limited similarity, more like kernel methods (Chapter 6)

7. Data encoding crucial: Feature map determines model power

</section>