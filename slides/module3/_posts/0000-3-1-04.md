<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w7.4 Quantum circuits and NNs <!-- .element: class="r-fit-text" -->

- [w7.4.1] Emulating nonlinear activations via encoding
- [w7.4.2] Variational circuits as deep linear NNs  
- [w7.4.3] Time-evolution encoding as exponential activation

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1 - Nonlinear activations (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

The term "quantum neural networks" for variational circuits is partially misleading. The essence of neural networks is their structure of alternating linear transformations and element-wise nonlinear activations (multi-layer perceptrons), which quantum circuits do not naturally exhibit.

</div>

**Challenge:** How to introduce nonlinearities $v \to \phi(v)$ into quantum circuits?

**Key insight:** Options strongly depend on the encoding strategy chosen.

**Three encoding approaches:**
1. Amplitude encoding
2. Basis encoding
3. Rotation encoding

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1 - Nonlinear activations (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Different encoding strategies offer different trade-offs for implementing nonlinear activation functions.

</div>

**Amplitude encoding:**
- ✓ Natural quantum representation
- ✗ Requires measurements + branch selection
- ✗ Costly representation switching

**Basis encoding:**
- ✓ Deterministic nonlinearities
- ✓ Simple step/ReLU implementation
- ✓ Efficient sigmoid via Quine-McCluskey
- ✗ Requires classical-to-reversible conversion

**Rotation encoding:**
- ✓ Avoids full representation switching
- ✓ Repeat-until-success for sigmoid-like functions
- ✗ Non-deterministic; longer average runtime

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.2 - VCs as deep linear NNs <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Variational circuits can be interpreted as deep linear neural networks with specific structural properties.

</div>

**Layer interpretation:**
- **Input layer:** Amplitudes $\langle\mu_i|\phi(x)\rangle$ in measurement eigenbasis
- **Hidden layers:** Quantum gates as linear transformations (weight matrices)
- **Output layer:** Measurement as nonlinear activation $\phi(a) = |a|^2$

**Key structural properties:**
- Single-qubit gates create sparse $2^n \times 2^n$ matrices with **parameter tying** (shared weights)
- Controlled gates break symmetry, add identity connections
- Constant qubit number → constant layer width

<div style="text-align: justify;">

Quantum circuits implement specialized deep linear networks with structured connectivity from tensor product structure.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.3 - Time-evolution as activation <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

The Fourier formalism reveals quantum models can be interpreted as single-hidden-layer neural networks with exponential activation.

</div>

**From time-evolution encoding** $S_i(x_i) = e^{-ix_iG_i}$:

$$f_\theta(x) = \sum_{\omega \in \Omega} c_\omega(\theta) e^{i\omega \cdot x}$$

**Neural network form:**

$$f_{w,W}(x) = \sum_{j=1}^{J} w_j \phi(W_j \cdot x) \quad \text{with activation } \phi(a) = e^{ia}$$

- **First layer weights $W$:** Frequency vectors (fixed by encoding Hamiltonians)
- **Second layer weights $w$:** Fourier coefficients (trainable parameters)

**Connection to random Fourier features:** Links quantum models to kernel methods (SVMs).

</section>