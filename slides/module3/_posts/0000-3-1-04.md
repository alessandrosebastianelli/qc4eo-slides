<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w7.4 Quantum circuits and NNs <!-- .element: class="r-fit-text" -->

- [w7.4.1] Emulating nonlinear activations via encoding
- [w7.4.2] Variational circuits as deep linear NNs  
- [w7.4.3] Time-evolution encoding as exponential activation

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1 - Nonlinear activations (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

The term "quantum neural networks" for variational circuits is partially misleading. The essence of neural networks is their structure of alternating linear transformations and element-wise nonlinear activations (multi-layer perceptrons), which quantum circuits do not naturally exhibit.

</div>

**Challenge:** How to introduce nonlinearities $v \to \phi(v)$ into quantum circuits?

**Key insight:** Options strongly depend on the encoding strategy chosen.

**Three encoding approaches:**
1. Amplitude encoding
2. Basis encoding
3. Rotation encoding

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.1 - Amplitude encoding <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

In amplitude encoding, neural network inputs are represented as amplitudes of a quantum state. The goal is to transform each amplitude $v_i$ to $\phi(v_i)$.

</div>

**Fundamental limitation:** Nonlinear transformations of amplitudes are impossible with unitary evolution alone.

**Only nonlinear element in quantum theory:** Measurements (but outcomes are non-deterministic).

**Solution using branch selection:**

$$|0\rangle|v_i\rangle|i\rangle \to \left(\phi(v_i)|0\rangle + \sqrt{1-\phi(v_i)^2}|1\rangle\right)|v_i\rangle|i\rangle$$

<div style="text-align: justify;">

Then branch selection plus uncomputing $|v_i\rangle$ yields state proportional to $\phi(v_i)|i\rangle$. However, moving between representations is costly, making this approach questionable for feed-forward networks.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.2 - Basis encoding (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

In basis encoding, classical values are directly encoded into computational basis states. Nonlinear transformations $|v\rangle|0\rangle \to |v\rangle|\phi(v)\rangle$ become straightforward to implement deterministically.

</div>

**Fixed-point arithmetic representation:**

Input $v$ represented as binary sequence:

$$b_s b_{\tau_l-1} \cdots b_1 b_0 \cdot b_{-1} \cdots b_{-\tau_r}$$

where $b_s$ is the sign bit, $\tau_l$ bits left of decimal, $\tau_r$ bits right of decimal.

Real value retrieved via:

$$v = (-1)^{b_s}\left(b_{\tau_l-1}2^{\tau_l-1} + \cdots + b_0 2^0 + b_{-1}2^{-1} + \cdots + b_{-\tau_r}2^{-\tau_r}\right)$$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.2 - Basis encoding (2) <!-- .element: class="r-fit-text" -->

**Step function:**

$$\phi(v) = \begin{cases} 0 & \text{if } v \leq 0 \\ 1 & \text{else} \end{cases}$$

<div style="text-align: justify;">

**Implementation:** Trivial - simply read the sign bit $b_s$ of the input register as the output. No additional qubits needed.

</div>

**Rectified Linear Unit (ReLU):**

$$\phi(v) = \begin{cases} 0 & \text{if } v \geq 0 \\ v & \text{else} \end{cases}$$

<div style="text-align: justify;">

**Implementation:** Conditional on sign bit $b_s$, either copy input register to output register (using controlled-NOT gates) or do nothing to represent zero.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.2 - Basis encoding (3) <!-- .element: class="r-fit-text" -->

**Sigmoid function:**

$$\phi(v; \delta) = \frac{1}{1 + e^{-\delta v}}$$

<div style="text-align: justify;">

**Challenge:** Direct implementation via division and exponential is demanding in both elementary operations and spatial resources.

</div>

**More efficient approaches:**
- **Piecewise linear approximation:** Approximate sigmoid with linear segments
- **Boolean function (lookup table):** For low precision, explicitly define output bits for every possible input

**Quine-McCluskey method:** Optimize boolean function by recognizing patterns, significantly reducing the number of control qubits needed.

<div style="text-align: justify;">

Result: Only 6 fractional bits provide smooth approximation for sigmoid scaled to $[-1,1]$ interval.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.3 - Rotation encoding (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Rotation encoding lies between amplitude and basis encoding for implementing nonlinearities. It requires measurements but avoids costly representation switching.

</div>

**Setup:** Net input $v$ encoded via Pauli-Y rotation into an ancilla qubit:

$$R_y(2v)|0\rangle \otimes |\psi\rangle_{\text{out}} \quad \text{where} \quad R_y(2v)|0\rangle = \cos v|0\rangle + \sin v|1\rangle$$

**Goal:** Rotate the output qubit by a nonlinear function of $v$, preparing state $R_y(2\phi(v))|\psi\rangle$.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.1.3 - Rotation encoding (2) <!-- .element: class="r-fit-text" -->

**Repeat-until-success circuits:**

<div style="text-align: justify;">

An elegant probabilistic method for implementing angle-encoded activations:

</div>

1. Apply evolution to ancillas and output qubit
2. Measure ancilla qubit:
   - Outcome $|0\rangle$ → desired output state achieved, continue
   - Outcome $|1\rangle$ → reset circuit and repeat evolution
3. Continue until ancilla measured in $|0\rangle$

**Example:** Sigmoid-like activation $\phi(v) = \arctan(\tan^2(v))$ can be implemented this way.

<div style="text-align: justify;">

**Advantage:** Unlike branch selection, don't need to repeat entire routine before the evolution step, only the local evolution itself. However, this is non-deterministic with average runtime slightly longer than classical feed-forward pass.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.2 - VCs as deep linear NNs (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

We now examine how standard variational circuits can be interpreted as neural networks, revealing interesting structural similarities despite the lack of explicit perceptron layers.

</div>

**Interpretation approach:** Cast deterministic quantum model into neural network notation.

**Basis choice:** Use measurement eigenbasis $\{|\mu_i\rangle\}$ where $M = \sum_i \mu_i|\mu_i\rangle\langle\mu_i|$ is diagonal.

**Layer structure:**
- **Input layer:** Amplitudes $\langle\mu_i|\phi(x)\rangle$ serve as input neuron values
- **Hidden layers:** Quantum gates are linear transformations (trainable or fixed weight matrices)
- **Output layer:** Measurement introduces nonlinearity with activation $\phi(a) = |a|^2$, followed by linear combination with weights $\mu_i$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.2 - VCs as deep linear NNs (2) <!-- .element: class="r-fit-text" -->

**Key observations:**

<div style="text-align: justify;">

If qubit number is kept constant throughout the computation, the corresponding neural network has layers of constant width. Layer width can be varied by adding qubits or excluding some from measurement.

</div>

**Connectivity structure:** A single-qubit gate acting on the $i$-th qubit of $n$ qubits:

$$G_{qi} = \mathbb{1}_1 \otimes \cdots \otimes \underbrace{G}_{\text{position } i} \otimes \cdots \otimes \mathbb{1}_n$$

<div style="text-align: justify;">

results in a sparse $2^n \times 2^n$ matrix. The sparsity pattern depends on which qubit the gate acts on, creating a highly symmetric structure with extensive **parameter tying** (multiple connections sharing the same weight).

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.2 - VCs as deep linear NNs (3) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Controlled gates break this symmetry by replacing some weight connections with identity mappings that simply carry over values from the previous layer. This creates a hierarchical structure where:

</div>

- **Single-qubit gates:** Connect sets of variables with same weights (high parameter tying)
- **Controlled gates:** Selectively break ties, replacing some connections with identities

<div style="text-align: justify;">

This perspective reveals that quantum circuits implement a specialized form of deep linear network with structured connectivity patterns determined by the tensor product structure of quantum mechanics. While not as flexible as arbitrary neural networks, this structure may offer computational advantages.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.3 - Time-evolution as activation (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

The second neural network interpretation stems from the Fourier formalism developed in Section w7.2. This connection links quantum models to an important class of classical machine learning methods.

</div>

**Recall:** For time-evolution encoding $S_i(x_i) = e^{-ix_iG_i}$, quantum models have the form:

$$f_\theta(x) = \sum_{\omega \in \Omega} c_\omega(\theta) e^{i\omega \cdot x}$$

<div style="text-align: justify;">

where $\omega$ are frequency vectors determined by the encoding generators, and $c_\omega(\theta)$ are Fourier coefficients depending on the variational parameters.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.3 - Time-evolution as activation (2) <!-- .element: class="r-fit-text" -->

**Interpret as single-hidden-layer neural network:**

$$f_{w,W}(x) = \sum_{j=1}^{J} w_j \phi(W_j \cdot x)$$

with complex exponential hidden-layer activation function $\phi(a) = e^{ia}$.

**Weight structure:**
- **Input weights $W$:** Rows are frequency vectors (fixed by encoding Hamiltonians)
- **Output weights $w$:** Fourier coefficients $c_\omega$ (trainable, depend on circuit parameters $\theta$)

<div style="text-align: justify;">

This is a restricted neural network where the first layer weights are fixed by physics (the encoding strategy) and only the second layer is optimized.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.4.3 - Time-evolution as activation (3) <!-- .element: class="r-fit-text" -->

**Connection to random Fourier features:**

<div style="text-align: justify;">

This structure closely resembles random Fourier feature models, where frequency vectors are sampled from a distribution. A powerful result shows that the Fourier transform of that distribution corresponds to a kernel function, meaning the random Fourier feature model approximates a kernel method (like support vector machines).

</div>

**Why this matters:**
- Links kernel methods and neural networks
- Extensively exploited in deep learning theory
- Suggests quantum models may have natural kernel interpretations (confirmed in next chapter)

**Conclusion:** While variational circuits don't naturally exhibit multi-perceptron structure, they connect to neural network theory through Fourier analysis and random features.

</section>