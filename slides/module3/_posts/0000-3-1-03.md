<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w7.3 Training VQMs <!-- .element: class="r-fit-text" -->

- [w7.3.1] Gradients of quantum computations
- [w7.3.2] Parameter-shift rules
- [w7.3.3] Barren plateaus
- [w7.3.4] Generative training

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Training a variational quantum model means finding the parameters $\theta$ that minimize a data-dependent cost function. The most common approach uses automatic differentiation, a powerful technique from deep learning that computes gradients efficiently.

</div>

**Automatic differentiation:** For a cost function $C(\theta)$ that depends on model $f_\theta$, the chain rule gives:

$$\partial_\mu C(\theta) = \frac{\partial C}{\partial f_\theta} \frac{\partial f_\theta}{\partial \mu}$$

<div style="text-align: justify;">

The classical part $\frac{\partial C}{\partial f_\theta}$ can be handled by standard automatic differentiation libraries. The challenge is computing $\frac{\partial f_\theta}{\partial \mu}$, the partial derivative of a quantum computation.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (2) <!-- .element: class="r-fit-text" -->

**Finite differences approach:**

<div style="text-align: justify;">

One could approximate the derivative numerically using finite differences:

</div>

$$\frac{\partial f_\theta}{\partial \mu} \approx \frac{f_\theta - f_{\theta+\Delta\theta}}{\Delta\mu}$$

<div style="text-align: justify;">

where $\Delta\mu$ is an infinitesimal shift. However, this is problematic in quantum computing because each function evaluation can only be estimated with error. The smaller the gradient, the more precision (and thus more measurements) needed to resolve the signal.

</div>

**This creates particular difficulties when:**
- The minimum must be approximated closely
- The optimization landscape has many saddle points
- Measurements have high variance

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (3) <!-- .element: class="r-fit-text" -->

**The analytical gradient challenge:**

<div style="text-align: justify;">

Consider a quantum model $f_\mu = \langle\psi|G^\dagger(\mu)BG(\mu)|\psi\rangle$ depending on parameter $\mu$ affecting gate $G(\mu)$. By linearity, the partial derivative is:

</div>

$$\partial_\mu f_\mu = \langle\psi|G^\dagger B(\partial_\mu G)|\psi\rangle + \langle\psi|(\partial_\mu G)^\dagger BG|\psi\rangle$$

**Fundamental problem:**
- Each term is NOT a quantum expectation value (bra state $\neq$ ket state)
- The derivative $\partial_\mu G$ is not necessarily unitary
- Unclear how to evaluate using quantum computation

<div style="text-align: justify;">

This seems to preclude computing analytical gradients on quantum computers. Fortunately, parameter-shift rules provide an elegant solution.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (1) <!-- .element: class="r-fit-text" -->

**Definition 5.4 (Parameter-shift rule)**

<div style="text-align: justify;">

Let $f_\mu = \langle M \rangle_\mu$ be a quantum expectation value depending on parameter $\mu$. A parameter-shift rule is an identity expressing the derivative as a linear combination of expectation values at shifted parameters:

</div>

$$\partial_\mu f_\mu = \sum_i a_i f_{\mu+s_i}$$

where $\{a_i\}$ and $\{s_i\}$ are real scalar coefficients and shifts.

**Key advantages:**
- The gradient is **exact**, not approximated
- The shifts $s_i$ are not infinitesimal (often $\pi/2$)
- Evaluates to the analytic gradient using quantum hardware

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (2) <!-- .element: class="r-fit-text" -->

**Most prominent rule:** 

<div style="text-align: justify;">

For gates generated by operators satisfying $G^2 = \mathbb{1}$ (such as Pauli rotations), the gate can be written as:

</div>

$$G(\mu) = e^{-i\mu G} = \cos(\mu)\mathbb{1} - i\sin(\mu)G$$

<div style="text-align: justify;">

Under unitary conjugation by this gate, the measurement operator becomes:

</div>

$$K(\mu) = G^\dagger(\mu)BG(\mu) = A + B\cos(\mu) + C\sin(\mu)$$

where operators $A$, $B$, $C$ are independent of $\mu$.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (3) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Using trigonometric identities, for any shift $s \neq k\pi$ with $k \in \mathbb{Z}$:

</div>

$$\frac{\partial \cos(\mu)}{\partial \mu} = \frac{\cos(\mu+s) - \cos(\mu-s)}{2\sin(s)}$$

$$\frac{\partial \sin(\mu)}{\partial \mu} = \frac{\sin(\mu+s) - \sin(\mu-s)}{2\sin(s)}$$

<div style="text-align: justify;">

Applying these to $K(\mu)$ and taking expectations gives the two-term parameter-shift rule:

</div>

$$\partial_\mu f_\mu = \frac{1}{2\sin(s)}\left(f_{\mu+s} - f_{\mu-s}\right)$$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (4) <!-- .element: class="r-fit-text" -->

**Common choice:** Setting $s = \pi/2$ simplifies the formula:

$$\partial_\mu f_\mu = \frac{1}{2}\left(f_{\mu+\pi/2} - f_{\mu-\pi/2}\right)$$

**Extensions:**
- **Higher derivatives:** Chain parameter-shift rules to compute Hessians and Fubini-Study metric tensors
- **Multiple gates:** If parameter appears in $j$ gates, apply rule to each independently and sum
- **Non-projector gates:** More general rules exist with additional terms
- **Stochastic variants:** Sample shifts with expected value equal to analytic gradient

<div style="text-align: justify;">

These rules enable seamless integration of quantum computing into hybrid classical-quantum machine learning pipelines built with PyTorch or TensorFlow.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (5) <!-- .element: class="r-fit-text" -->

**Computational scaling:**

Standard two-term rules require $2K \times S$ circuit evaluations where:
- $K$: number of trainable parameters
- $S$: shots (measurements) per expectation value

**Good news for shots:** Can use very small $S$ (even $S=1$) in early training stages because quantum expectations give unbiased gradient estimators. Many low-shot iterations still converge.

**Challenge with parameters:** Scaling is linear in $K$. Each gradient element computed separately, unlike neural network backpropagation which computes full gradient in roughly one evaluation by reusing intermediate results.

<div style="text-align: justify;">

The quantum no-cloning theorem prevents sharing backpropagated information, making improved gradient methods an active research area.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Having parameter-shift rules does not guarantee successful training. The optimization landscape itself may be unfavorable. Barren plateaus are regions where gradients are exponentially small with high probability, making gradient-based optimization extremely slow.

</div>

**Definition:** Areas in the parameter space where $\nabla_\theta f_\theta \approx 0$ with high probability.

**Consequences:**
- Optimization becomes prohibitively slow
- Need exponentially high precision measurements to resolve tiny gradients
- Require $O(\epsilon^{-2})$ shots for error $\epsilon$, so exponentially small gradients need exponentially many measurements

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Barren plateaus often occur when variational circuits are highly expressive and Hilbert spaces are large. The intuition is geometric: in an exponentially large space, random rotations make any single parameter's effect on the measurement negligible on average.

</div>

**Observed when circuits "scramble" information via:**
- Sufficiently deep circuits
- Global measurements (equivalent to deep processing before local measurement)
- High noise levels
- High entanglement entropy with other systems

**Design principle:** Find limited, structured ansätze that restrict training to relevant subspaces of the vast Hilbert space rather than exploring randomly.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (3) <!-- .element: class="r-fit-text" -->

**McClean et al. foundational result:**

<div style="text-align: justify;">

For circuits drawn uniformly at random according to the Haar measure (or more generally, from 2-designs), the variance of the gradient decreases exponentially with the number of qubits.

</div>

**Definition 5.5 (t-design):** A distribution $\{p_k, W_k\}$ over unitaries is a t-design if averages over polynomials of degree up to $t$ in the unitary elements match Haar measure averages:

$$\sum_k p_k P_t(W_k, W_k^*) = \int P_t(W, W^*)d\mu(W)$$

<div style="text-align: justify;">

Since gradient variance is a second moment, the result applies to 2-designs, which are easier to implement than true Haar-random unitaries.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (4) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Consider expectation $f_\mu = \langle 0|W(\mu)^\dagger MW(\mu)|0\rangle$ with circuit $W(\mu) = BGG'A$ where $G(\mu) = e^{-i\mu G}$ and $A, B$ are sampled from a 2-design.

The variance of the gradient:

</div>

$$\text{Var}[\partial_\mu f] = \langle(\partial_\mu f)^2\rangle_W - \langle\partial_\mu f\rangle_W^2$$

<div style="text-align: justify;">

can be evaluated using the parameter-shift rule and Haar integral identities. After lengthy but straightforward calculation:

</div>

$$\text{Var}[\partial_\mu f_\mu] = \frac{g(Z, G, M)}{2^{2n} - 1}$$

where $g$ typically scales as $O(2^n)$.

**Conclusion:** Variance $\sim O(2^{-n})$ → exponential decay in qubit number.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.4 - Generative training (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

The automatic differentiation rules we derived for deterministic models also apply to probabilistic quantum models, enabling training of generative models and density estimators.

</div>

**Key insight:** The probability distribution $p_\theta(x)$ can be interpreted as a quantum expectation value.

<div style="text-align: justify;">

For a generative model $p_\mu(x)$ depending on parameter $\mu$, consider the expectation of a function $g(x)$ over samples:

</div>

$$\mathbb{E}_{x \sim p_\mu}[g(x)] = \int_{\mathcal{X}} p_\mu(x)g(x)dx$$

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.4 - Generative training (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

If the circuit allows a Pauli-gate parameter-shift rule, the partial derivative of this expectation can be computed by sampling from shifted distributions:

</div>

$$\partial_\mu \mathbb{E}_{x \sim p_\mu}[g(x)] = \frac{1}{2\sin(s)}\left[\mathbb{E}_{x \sim p_{\mu-s}}[g(x)] - \mathbb{E}_{x \sim p_{\mu+s}}[g(x)]\right]$$

**In practice:** Estimate the derivative by drawing samples from two generative models with parameters $\mu \pm s$ and evaluating $g$ on these samples.

**Application:** This enables training via two-sample testing methods that minimize the maximum mean discrepancy, which measures the statistical distance between distributions.

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.4 - Generative training (3) <!-- .element: class="r-fit-text" -->

**Generative adversarial networks (GANs):**

<div style="text-align: justify;">

When parameter-shift rules are not available or practical, we can adapt the GAN framework to quantum models. GANs train generative models through an adversarial game between two networks:

</div>

- **Generator $G(\theta_G)$:** Variational circuit as generative model, creates samples
- **Discriminator $D(\theta_D)$:** Variational circuit as classifier, distinguishes real from generated samples

<div style="text-align: justify;">

The generator learns to fool the discriminator, while the discriminator learns to detect generated samples. Both improve through this competition.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.4 - Generative training (4) <!-- .element: class="r-fit-text" -->

**Quantum twist:**

<div style="text-align: justify;">

If the real data is produced by a quantum circuit $C$ (e.g., simulating a quantum system), we can skip the measurement step entirely. Connect either $C$ or generator $G$ directly to discriminator circuit $D$, followed by a simple measurement (e.g., of a designated qubit).

</div>

**Training costs:**

Discriminator: $C_D(\theta) = p(1|G, \theta) - p(1|C, \theta)$

Generator: $C_G(\theta) = -p(1|G)$

**Applications beyond generative modeling:**
- Circuit approximation (compress large circuit to shorter one)
- State learning (learn to prepare target quantum state)
- Different cost functions possible (e.g., quantum earth mover's distance)

</section>