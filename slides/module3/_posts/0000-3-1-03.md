<!-- .slide: data-background="#ffffffff" -->

<section data-transition="none">

### w7.3 Training VQMs <!-- .element: class="r-fit-text" -->

- [w7.3.1] Gradients of quantum computations
- [w7.3.2] Parameter-shift rules
- [w7.3.3] Barren plateaus

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Training a variational quantum model means finding the parameters $\theta$ that minimize a data-dependent cost function. The most common approach uses automatic differentiation, a powerful technique from deep learning that computes gradients efficiently.

</div>

**Automatic differentiation:** For a cost function $C(\theta)$ that depends on model $f_\theta$, the chain rule gives:

$$\partial_\mu C(\theta) = \frac{\partial C}{\partial f_\theta} \frac{\partial f_\theta}{\partial \mu}$$

<div style="text-align: justify;">

The classical part $\frac{\partial C}{\partial f_\theta}$ can be handled by standard automatic differentiation libraries. The challenge is computing $\frac{\partial f_\theta}{\partial \mu}$, the partial derivative of a quantum computation.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (2) <!-- .element: class="r-fit-text" -->

**Finite differences approach:**

<div style="text-align: justify;">

One could approximate the derivative numerically using finite differences:

</div>

$$\frac{\partial f_\theta}{\partial \mu} \approx \frac{f_\theta - f_{\theta+\Delta\theta}}{\Delta\mu}$$

<div style="text-align: justify;">

where $\Delta\mu$ is an infinitesimal shift. However, this is problematic in quantum computing because each function evaluation can only be estimated with error. The smaller the gradient, the more precision (and thus more measurements) needed to resolve the signal.

</div>

**This creates particular difficulties when:**
- The minimum must be approximated closely
- The optimization landscape has many saddle points
- Measurements have high variance

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.1 - Gradients (3) <!-- .element: class="r-fit-text" -->

**The analytical gradient challenge:**

<div style="text-align: justify;">

Consider a quantum model $f_\mu = \langle\psi|G^\dagger(\mu)BG(\mu)|\psi\rangle$ depending on parameter $\mu$ affecting gate $G(\mu)$. By linearity, the partial derivative is:

</div>

$$\partial_\mu f_\mu = \langle\psi|G^\dagger B(\partial_\mu G)|\psi\rangle + \langle\psi|(\partial_\mu G)^\dagger BG|\psi\rangle$$

**Fundamental problem:**
- Each term is NOT a quantum expectation value (bra state $\neq$ ket state)
- The derivative $\partial_\mu G$ is not necessarily unitary
- Unclear how to evaluate using quantum computation

<div style="text-align: justify;">

This seems to preclude computing analytical gradients on quantum computers. Fortunately, parameter-shift rules provide an elegant solution.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (1) <!-- .element: class="r-fit-text" -->

**Definition 5.4 (Parameter-shift rule)**

<div style="text-align: justify;">

Let $f_\mu = \langle M \rangle_\mu$ be a quantum expectation value depending on parameter $\mu$. A parameter-shift rule is an identity expressing the derivative as a linear combination of expectation values at shifted parameters:

</div>

$$\partial_\mu f_\mu = \sum_i a_i f_{\mu+s_i}$$

where $\{a_i\}$ and $\{s_i\}$ are real scalar coefficients and shifts.

**Key advantages:**
- The gradient is **exact**, not approximated
- The shifts $s_i$ are not infinitesimal (often $\pi/2$)
- Evaluates to the analytic gradient using quantum hardware

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.2 - Parameter-shift rules (2) <!-- .element: class="r-fit-text" -->

**Computational scaling:**

<div style="text-align: justify;">

Standard two-term rules require $2K \times S$ circuit evaluations where:
- $K$: number of trainable parameters
- $S$: shots (measurements) per expectation value

**Good news for shots:** Can use very small $S$ (even $S=1$) in early training stages because quantum expectations give unbiased gradient estimators. Many low-shot iterations still converge.

**Challenge with parameters:** Scaling is linear in $K$. Each gradient element computed separately, unlike neural network backpropagation which computes full gradient in roughly one evaluation by reusing intermediate results.


The quantum no-cloning theorem prevents sharing backpropagated information, making improved gradient methods an active research area.

</div>

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (1) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Having parameter-shift rules does not guarantee successful training. The optimization landscape itself may be unfavorable. Barren plateaus are regions where gradients are exponentially small with high probability, making gradient-based optimization extremely slow.

</div>

**Consequences:**
- Optimization becomes prohibitively slow
- Need exponentially high precision measurements to resolve tiny gradients
- Require $O(\epsilon^{-2})$ shots for error $\epsilon$, so exponentially small gradients need exponentially many measurements

</section>

<!-- ============================================================================ -->

<section data-transition="none">

### w7.3.3 - Barren plateaus (2) <!-- .element: class="r-fit-text" -->

<div style="text-align: justify;">

Barren plateaus often occur when variational circuits are highly expressive and Hilbert spaces are large. The intuition is geometric: in an exponentially large space, random rotations make any single parameter's effect on the measurement negligible on average.



**Observed when circuits "scramble" information via:**
- Sufficiently deep circuits
- Global measurements (equivalent to deep processing before local measurement)
- High noise levels
- High entanglement entropy with other systems

**Design principle:** Find limited, structured ans√§tze that restrict training to relevant subspaces of the vast Hilbert space rather than exploring randomly.

</div>

</section>
