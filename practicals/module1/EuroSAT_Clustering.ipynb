{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amZN5j_oeLlM"
      },
      "source": [
        "# üõ∞Ô∏è Clustering Earth Observation Data: K-Means vs BIRCH\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this practical, we will explore two popular **unsupervised learning algorithms** ‚Äî **K-Means** and **BIRCH** ‚Äî and apply them to the **EuroSAT RGB dataset**. The goal is to identify natural groupings (clusters) in satellite imagery without using labels.\n",
        "\n",
        "### Why Clustering Earth Observation Data?\n",
        "Satellite data is abundant and often unlabeled. Clustering helps discover structure in this data, e.g., separating vegetation, urban areas, or water bodies ‚Äî useful in land-use mapping and environmental monitoring.\n",
        "\n",
        "### What We'll Learn\n",
        "- Load and preprocess EuroSAT RGB data\n",
        "- Reduce data dimensionality with **PCA**\n",
        "- Apply **K-Means** and **BIRCH** clustering\n",
        "- Compare performance and visualize clusters\n",
        "\n",
        "### Dataset Overview\n",
        "The **EuroSAT RGB** dataset contains 27,000 Sentinel-2 images (64√ó64√ó3) representing 10 land-cover classes:\n",
        "- Annual Crop ‚ÄÉ- Forest‚ÄÉ- Herbaceous Vegetation‚ÄÉ- Highway‚ÄÉ- Industrial  \n",
        "- Pasture‚ÄÉ- Permanent Crop‚ÄÉ- Residential‚ÄÉ- River‚ÄÉ- Sea/Lake\n",
        "\n",
        "[EuroSAT dataset](https://github.com/phelber/EuroSAT)\n",
        "\n",
        "![](https://raw.githubusercontent.com/phelber/EuroSAT/master/eurosat_overview_small.jpg)"
      ],
      "id": "amZN5j_oeLlM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92cs17SAeLlN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, Birch\n",
        "import urllib.request, zipfile, os, time, warnings\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "id": "92cs17SAeLlN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKAmWpl0eLlO"
      },
      "source": [
        "## 2. Data Acquisition and Loading\n",
        "\n",
        "We'll download the EuroSAT RGB dataset from **Zenodo** and extract a manageable subset of images for this lesson (e.g., 300 per class). Each image is 64√ó64 pixels with three color channels (RGB)."
      ],
      "id": "YKAmWpl0eLlO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdMLHQ73eLlO"
      },
      "outputs": [],
      "source": [
        "def download_eurosat():\n",
        "    \"\"\"\n",
        "    Download the official EuroSAT RGB dataset from DFKI (SSL verification disabled).\n",
        "    \"\"\"\n",
        "    url = \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\"\n",
        "    zip_path = \"EuroSAT.zip\"\n",
        "    target_dir = \"EuroSAT/2750\"\n",
        "\n",
        "    if not os.path.exists(target_dir):\n",
        "        print(\"‚¨áÔ∏è Downloading EuroSAT RGB dataset from DFKI...\")\n",
        "        response = requests.get(url, verify=False, stream=True)\n",
        "        total = int(response.headers.get('content-length', 0))\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            downloaded = 0\n",
        "            for data in response.iter_content(chunk_size=8192):\n",
        "                f.write(data)\n",
        "                downloaded += len(data)\n",
        "                done = int(50 * downloaded / total)\n",
        "                print(f\"\\r[{'=' * done}{' ' * (50 - done)}] {downloaded/1e6:.1f}/{total/1e6:.1f} MB\", end='')\n",
        "        print(\"\\nüì¶ Extracting dataset...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"EuroSAT\")\n",
        "        os.remove(zip_path)\n",
        "        print(\"‚úÖ Dataset downloaded and extracted successfully!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Dataset already available.\")\n",
        "\n",
        "def load_images(data_dir=\"EuroSAT/2750\", max_per_class=300):\n",
        "    \"\"\"\n",
        "    Load RGB images from the EuroSAT dataset.\n",
        "    Each image is 64x64x3.\n",
        "    \"\"\"\n",
        "    images, labels, classes = [], [], []\n",
        "    for class_dir in sorted(Path(data_dir).iterdir()):\n",
        "        if class_dir.is_dir():\n",
        "            cls = class_dir.name\n",
        "            classes.append(cls)\n",
        "            files = list(class_dir.glob(\"*.jpg\"))[:max_per_class]\n",
        "            for f in files:\n",
        "                img = np.array(Image.open(f))\n",
        "                images.append(img)\n",
        "                labels.append(cls)\n",
        "    return np.array(images), np.array(labels), classes\n",
        "\n",
        "\n",
        "# Run download + load\n",
        "download_eurosat()\n",
        "images, true_labels, class_names = load_images(max_per_class=300)\n",
        "print(f\"‚úÖ Loaded {len(images)} images from {len(class_names)} classes.\")\n"
      ],
      "id": "wdMLHQ73eLlO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgFtntAoeLlO"
      },
      "source": [
        "## 3. Exploring the Dataset\n",
        "\n",
        "Let's take a quick look at the data by plotting one sample from each class. Visual inspection helps confirm that the dataset loaded correctly and provides intuition for clustering."
      ],
      "id": "EgFtntAoeLlO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkosR9BoeLlO"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 5, figsize=(15,6))\n",
        "fig.suptitle('Example Images from EuroSAT (RGB)', fontsize=14, fontweight='bold')\n",
        "for i, cls in enumerate(class_names):\n",
        "    idx = np.where(true_labels == cls)[0][0]\n",
        "    axes[i//5, i%5].imshow(images[idx])\n",
        "    axes[i//5, i%5].set_title(cls.replace('_',' '))\n",
        "    axes[i//5, i%5].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ZkosR9BoeLlO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcULg1A4eLlO"
      },
      "source": [
        "## 4. Dimensionality Reduction with PCA\n",
        "\n",
        "Each image has 64√ó64√ó3 = **12,288 features**. Clustering directly in this high-dimensional space is computationally expensive and prone to the *curse of dimensionality*.\n",
        "\n",
        "We use **Principal Component Analysis (PCA)** to:\n",
        "- Reduce redundancy among correlated features (RGB pixels)\n",
        "- Keep the components explaining most variance\n",
        "- Make clustering faster and more effective\n",
        "\n",
        "We'll project down to **50 components**, typically capturing ~90% of the data variance."
      ],
      "id": "HcULg1A4eLlO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7tdOrzPeLlP"
      },
      "outputs": [],
      "source": [
        "X = images.reshape(len(images), -1) / 255.0\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(f\"Reduced from {X.shape[1]} to {X_pca.shape[1]} features.\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.2%}\")"
      ],
      "id": "y7tdOrzPeLlP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgbfKOQeLlP"
      },
      "source": [
        "Let's check how much variance each PCA component captures. This helps justify our choice of 50 components."
      ],
      "id": "CLgbfKOQeLlP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJU4t9OheLlP"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True, alpha=0.4)\n",
        "plt.show()"
      ],
      "id": "KJU4t9OheLlP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6j0l32eeLlP"
      },
      "source": [
        "## 5. K-Means Clustering\n",
        "\n",
        "\n",
        "**What it is**:\n",
        "\n",
        "K-Means is a partition-based clustering algorithm. It tries to group your data into K clusters so that items in the same cluster are similar, and items in different clusters are as different as possible.\n",
        "\n",
        "\n",
        "**How it works (simplified):**\n",
        "\n",
        "Pick K random points as initial cluster centers (called centroids).\n",
        "\n",
        "Assign each data point to the nearest centroid.\n",
        "\n",
        "Update each centroid to be the mean of all points assigned to it.\n",
        "\n",
        "Repeat steps 2‚Äì3 until centroids stop moving or a maximum number of iterations is reached.\n",
        "\n",
        "\n",
        "**Key points:**\n",
        "\n",
        "You must decide the number of clusters (K) beforehand.\n",
        "\n",
        "Works best when clusters are roughly spherical and similar in size.\n",
        "\n",
        "Sensitive to initial centroid positions ‚Äî running it multiple times with different seeds usually gives better results.\n",
        "\n",
        "Fast and widely used, but it might struggle with irregular cluster shapes.\n",
        "\n",
        "**Why use it here:**\n",
        "\n",
        "K-Means can help identify patterns in satellite images, like grouping areas with similar land cover (forest, river, urban, etc.), even if we don‚Äôt tell the algorithm what the classes are.\n",
        "\n",
        "**Algorithm Steps:**\n",
        "1. Initialize K centroids randomly\n",
        "2. Assign each point to its nearest centroid\n",
        "3. Update centroids as the mean of assigned points\n",
        "4. Repeat until convergence\n",
        "\n",
        "We‚Äôll set *K = 10* (same as the number of known classes) and evaluate clustering quality."
      ],
      "id": "v6j0l32eeLlP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goTbn5FqeLlP"
      },
      "outputs": [],
      "source": [
        "n_clusters = len(class_names)\n",
        "start = time.time()\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "labels_km = kmeans.fit_predict(X_pca)\n",
        "time_km = time.time() - start\n",
        "\n",
        "sil_km = silhouette_score(X_pca, labels_km)\n",
        "ch_km = calinski_harabasz_score(X_pca, labels_km)\n",
        "\n",
        "print(f\"K-Means completed in {time_km:.2f}s\")\n",
        "print(f\"Silhouette Score: {sil_km:.3f}, Calinski-Harabasz Score: {ch_km:.1f}\")"
      ],
      "id": "goTbn5FqeLlP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piQCpfk6eLlQ"
      },
      "source": [
        "### Interpreting Metrics\n",
        "- **Silhouette Score** ‚àà [-1, 1]: Higher values indicate better separation.\n",
        "- **Calinski‚ÄìHarabasz Index**: Larger values ‚Üí well-separated, compact clusters.\n",
        "\n",
        "These metrics don't rely on labels, so they‚Äôre ideal for unsupervised evaluation."
      ],
      "id": "piQCpfk6eLlQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THmVA8fveLlQ"
      },
      "source": [
        "## 6. BIRCH Clustering\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) is a hierarchical clustering algorithm designed for large datasets.\n",
        "\n",
        "**How it works (simplified):**\n",
        "\n",
        "Builds a CF Tree (Clustering Feature Tree) that summarizes all points in the dataset.\n",
        "\n",
        "Each node in the tree keeps track of cluster statistics (like the number of points, their sum, and sum of squares).\n",
        "\n",
        "New points are incrementally added to the tree without storing all data in memory.\n",
        "\n",
        "After the tree is built, clusters can be extracted from the summarized nodes.\n",
        "\n",
        "**Key points:**\n",
        "\n",
        "Very memory-efficient; can handle large datasets that don‚Äôt fit entirely in memory.\n",
        "\n",
        "Can automatically discover clusters if n_clusters=None.\n",
        "\n",
        "Sensitive to the order of the data, so results might slightly change depending on the input order.\n",
        "\n",
        "Works well with large, high-dimensional data like images, especially after PCA.\n",
        "\n",
        "**Why use it here:**\n",
        "\n",
        "BIRCH is ideal for satellite images because it can quickly summarize and cluster thousands of images without consuming too much memory. It gives a different perspective from K-Means, allowing comparison of clustering strategies.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Handles massive data efficiently\n",
        "- Works incrementally\n",
        "- Can automatically discover number of clusters\n",
        "\n",
        "We'll again set `n_clusters=10` for comparability."
      ],
      "id": "THmVA8fveLlQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW3QT7wVeLlQ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "birch = Birch(n_clusters=n_clusters, threshold=0.5)\n",
        "labels_birch = birch.fit_predict(X_pca)\n",
        "time_br = time.time() - start\n",
        "\n",
        "sil_br = silhouette_score(X_pca, labels_birch)\n",
        "ch_br = calinski_harabasz_score(X_pca, labels_birch)\n",
        "\n",
        "print(f\"BIRCH completed in {time_br:.2f}s\")\n",
        "print(f\"Silhouette Score: {sil_br:.3f}, Calinski-Harabasz Score: {ch_br:.1f}\")"
      ],
      "id": "iW3QT7wVeLlQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40OWjiRKeLlQ"
      },
      "source": [
        "## 7. Comparing Algorithm Performance\n",
        "\n",
        "Let's summarize the metrics and compare runtime and clustering quality between K-Means and BIRCH."
      ],
      "id": "40OWjiRKeLlQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqGeWeOaeLlQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'Algorithm': ['K-Means', 'BIRCH'],\n",
        "    'Time (s)': [time_km, time_br],\n",
        "    'Silhouette Score': [sil_km, sil_br],\n",
        "    'Calinski-Harabasz': [ch_km, ch_br]\n",
        "})\n",
        "display(df)\n",
        "\n",
        "df.plot(x='Algorithm', y=['Time (s)', 'Silhouette Score'], kind='bar', subplots=True, layout=(1,2), figsize=(12,4), legend=False)\n",
        "plt.suptitle('Performance Comparison: K-Means vs BIRCH', fontweight='bold')\n",
        "plt.show()"
      ],
      "id": "mqGeWeOaeLlQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOuNJ9YzeLlQ"
      },
      "source": [
        "## 8. Visualizing Clusters in 2D\n",
        "\n",
        "To better interpret the clusters, we‚Äôll use another PCA projection to reduce our 50D data to 2D for plotting. Although the 2D projection can‚Äôt fully represent high-dimensional clusters, it offers a helpful visual intuition."
      ],
      "id": "wOuNJ9YzeLlQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOAfnxeveLlQ"
      },
      "outputs": [],
      "source": [
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_2d = pca_2d.fit_transform(X_pca)\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
        "ax[0].scatter(X_2d[:,0], X_2d[:,1], c=labels_km, cmap='tab10', s=10)\n",
        "ax[0].set_title('K-Means Clusters (2D PCA view)')\n",
        "ax[1].scatter(X_2d[:,0], X_2d[:,1], c=labels_birch, cmap='tab10', s=10)\n",
        "ax[1].set_title('BIRCH Clusters (2D PCA view)')\n",
        "plt.show()"
      ],
      "id": "cOAfnxeveLlQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUwKFkgfeLlQ"
      },
      "source": [
        "## 9. Inspecting Cluster Contents\n",
        "\n",
        "We‚Äôll visually inspect which kinds of images appear in each cluster. If clusters are semantically meaningful, similar landscapes (e.g., forests, water bodies) should appear together."
      ],
      "id": "QUwKFkgfeLlQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHh-mLUHeLlQ"
      },
      "outputs": [],
      "source": [
        "def show_cluster_samples(images, labels, algo, n_samples=5):\n",
        "    clusters = np.unique(labels)\n",
        "    fig, axes = plt.subplots(len(clusters), n_samples, figsize=(12,2.5*len(clusters)))\n",
        "    fig.suptitle(f'{algo}: Example Images per Cluster', fontsize=14, fontweight='bold')\n",
        "    for i, c in enumerate(clusters):\n",
        "        idxs = np.where(labels==c)[0]\n",
        "        idxs = np.random.choice(idxs, min(n_samples, len(idxs)), replace=False)\n",
        "        for j, idx in enumerate(idxs):\n",
        "            axes[i,j].imshow(images[idx])\n",
        "            axes[i,j].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_cluster_samples(images, labels_km, 'K-Means')\n",
        "show_cluster_samples(images, labels_birch, 'BIRCH')"
      ],
      "id": "JHh-mLUHeLlQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEum331veLlQ"
      },
      "source": [
        "## 10. Conclusions\n",
        "\n",
        "**Summary of Findings:**\n",
        "- PCA reduced image dimensionality effectively (~90% variance retained).\n",
        "- **K-Means** achieved slightly higher silhouette and CH scores ‚Äî better compactness.\n",
        "- **BIRCH** was faster and more scalable but sometimes less stable.\n",
        "\n",
        "**Takeaways:**\n",
        "- PCA is essential for high-dimensional EO data.\n",
        "- K-Means suits well-separated, compact clusters.\n",
        "- BIRCH scales better for large or streaming data.\n",
        "\n",
        "**Next Steps:**\n",
        "- Try tuning PCA components (20‚Äì100) and BIRCH threshold.\n",
        "- Experiment with subsets (urban vs. rural scenes).\n",
        "- Explore t-SNE or UMAP for nonlinear dimensionality reduction."
      ],
      "id": "rEum331veLlQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}